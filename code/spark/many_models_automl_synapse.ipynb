{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Many Model Training Using Synpase Spark 2.4 with AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment preperation\n",
        "1. Prepare a Synaspe Spark Pool 2.4\n",
        "As of this writing, Automl is not supported on Python 3.8 or later so we have to use Spark 2.4 for Automl\n",
        "2. To fix problem with Spark Pandas UDF's incompatbility with pyarrow >=0.15, we need to downgrade the  pyarrow environment to pyarrow 0.14.1. use the requirement file to downgrade pyarrow.\n",
        "3. Prepare a Azure ML workspace \n",
        "4. Prepare a service principal with secret key registered in keyvault. The service principal should have contributor access to your Azure ML workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download data from Microsoft Open Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T16:07:30.0180845Z",
              "execution_start_time": "2021-11-05T16:06:54.1550893Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T16:04:21.5906068Z",
              "session_id": 4,
              "session_start_time": "2021-11-05T16:04:21.616363Z",
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(spark001, 4, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data =spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://ojsales-simulatedcontainer@azureopendatastorage.blob.core.windows.net/oj_sales_data/Store10*.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:01:14.9855878Z",
              "execution_start_time": "2021-11-05T19:01:14.8388939Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:01:14.6836128Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 20
            },
            "text/plain": [
              "StatementMeta(spark001, 6, 20, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# #Write to local delta for fast reading\n",
        "data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"OJ_Sales_Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "microsoft": {
          "language": "sparksql"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T16:11:07.8786817Z",
              "execution_start_time": "2021-11-05T16:10:34.717225Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T16:10:34.5596943Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(spark001, 4, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [
                [
                  "1990-06-14",
                  "1097",
                  "minute.maid",
                  "9626",
                  "1",
                  "2.45",
                  "23583.7"
                ],
                [
                  "1990-06-21",
                  "1097",
                  "minute.maid",
                  "18695",
                  "1",
                  "2.31",
                  "43185.450000000004"
                ],
                [
                  "1990-06-28",
                  "1097",
                  "minute.maid",
                  "13535",
                  "1",
                  "2.1",
                  "28423.5"
                ],
                [
                  "1990-07-05",
                  "1097",
                  "minute.maid",
                  "17289",
                  "1",
                  "2.46",
                  "42530.94"
                ],
                [
                  "1990-07-12",
                  "1097",
                  "minute.maid",
                  "16015",
                  "1",
                  "1.99",
                  "31869.85"
                ],
                [
                  "1990-07-19",
                  "1097",
                  "minute.maid",
                  "13643",
                  "1",
                  "2.55",
                  "34789.649999999994"
                ],
                [
                  "1990-07-26",
                  "1097",
                  "minute.maid",
                  "15754",
                  "1",
                  "1.9",
                  "29932.6"
                ],
                [
                  "1990-08-02",
                  "1097",
                  "minute.maid",
                  "9762",
                  "1",
                  "1.9",
                  "18547.8"
                ],
                [
                  "1990-08-09",
                  "1097",
                  "minute.maid",
                  "11676",
                  "1",
                  "2.28",
                  "26621.28"
                ],
                [
                  "1990-08-16",
                  "1097",
                  "minute.maid",
                  "17712",
                  "1",
                  "2.41",
                  "42685.920000000006"
                ]
              ],
              "schema": {
                "fields": [
                  {
                    "metadata": {},
                    "name": "WeekStarting",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Store",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Brand",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Quantity",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Advert",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Price",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Revenue",
                    "nullable": true,
                    "type": "string"
                  }
                ],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 10 rows and 7 fields>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql \n",
        "select * from OJ_Sales_Data limit 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "microsoft": {
          "language": "sparksql"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T16:11:16.5583459Z",
              "execution_start_time": "2021-11-05T16:11:07.9718338Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T16:10:38.041537Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(spark001, 4, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [
                [
                  300
                ]
              ],
              "schema": {
                "fields": [
                  {
                    "metadata": {},
                    "name": "count(DISTINCT store, brand)",
                    "nullable": false,
                    "type": "long"
                  }
                ],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 1 rows and 1 fields>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql \n",
        "select count (distinct store, brand) from OJ_Sales_Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:44:25.009272Z",
              "execution_start_time": "2021-11-05T19:44:24.8595336Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:44:24.7656984Z",
              "session_id": 8,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(spark001, 8, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "spark.conf.set(' spark.sql.execution.arrow.maxRecordsPerBatch', 100)\n",
        "#Default is 10000 which in some cases may defeat the purpose of parallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###Many Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:46:15.9232838Z",
              "execution_start_time": "2021-11-05T19:46:15.77497Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:46:15.6944325Z",
              "session_id": 8,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 9
            },
            "text/plain": [
              "StatementMeta(spark001, 8, 9, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#prepare values to broadcast\n",
        "tenant_id ='' \n",
        "service_principal_id=''\n",
        "service_principal_password=''\n",
        "subscription_id = ''\n",
        "# Azure Machine Learning resource group NOT the managed resource group\n",
        "resource_group = '' \n",
        "\n",
        "#Azure Machine Learning workspace name, NOT Azure Databricks workspace\n",
        "workspace_name = ''  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test with a single store & brand combination (single time series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T18:33:31.2948317Z",
              "execution_start_time": "2021-11-05T18:33:00.2643352Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T18:33:00.1851455Z",
              "session_id": 6,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(spark001, 6, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Getting data\n",
        "import pandas as pd\n",
        "train_data_df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data where Store = '1066' and Brand ='tropicana'\").toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:48:48.8310109Z",
              "execution_start_time": "2021-11-05T19:48:47.0198004Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:48:46.936421Z",
              "session_id": 8,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 15
            },
            "text/plain": [
              "StatementMeta(spark001, 8, 15, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_data_df' is not defined",
          "output_type": "error",
          "traceback": [
            "NameError: name 'train_data_df' is not defined",
            "Traceback (most recent call last):\n",
            "NameError: name 'train_data_df' is not defined\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Model\n",
        "\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "import cloudpickle \n",
        "sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n",
        "                                         service_principal_id=service_principal_id,\n",
        "                                         service_principal_password=service_principal_password)\n",
        "# Instantiate Azure Machine Learning workspace\n",
        "ws = Workspace.get(name=workspace_name,\n",
        "                   subscription_id=subscription_id,\n",
        "                   resource_group=resource_group,auth= sp_auth)\n",
        "\n",
        "\n",
        "experiment_name = 'automl-ml-forecast-local'\n",
        "\n",
        "experiment=Experiment(ws, experiment_name)\n",
        "\n",
        "\n",
        "\n",
        "#Getting data for one table to test the utility function\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import joblib\n",
        "import os\n",
        "target_column= 'Quantity'\n",
        "timestamp_column= 'WeekStarting'\n",
        "timeseries_id_columns= [ 'Store', 'Brand']\n",
        "drop_columns=['Revenue', 'Store', 'Brand']\n",
        "model_type= 'lr'\n",
        "model_name=train_data_df['Store'][0]+\"_\"+train_data_df['Brand'][0]\n",
        "test_size=20\n",
        "# 1.0 Read the data from CSV - parse timestamps as datetime type and put the time in the index\n",
        "# data = train_data_df \\\n",
        "#         .set_index('WeekStarting') \\\n",
        "#         .sort_index(ascending=True)\n",
        "\n",
        "# 2.0 Split the data into train and test sets\n",
        "train = train_data_df[:-test_size]\n",
        "test = train_data_df[-test_size:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "time_column_name='WeekStarting'\n",
        "time_series_id_column_names=['Store', 'Brand']\n",
        "time_series_settings = {\n",
        "                        'time_column_name': time_column_name,\n",
        "                        'time_series_id_column_names': time_series_id_column_names,\n",
        "                        'forecast_horizon': 2\n",
        "                        }\n",
        "\n",
        "automl_config = AutoMLConfig(\n",
        "                                task = 'forecasting',\n",
        "                                debug_log='automl_oj_sales_errors.log',\n",
        "                                primary_metric='normalized_root_mean_squared_error',\n",
        "                                experiment_timeout_minutes=20,\n",
        "                                training_data=train,\n",
        "                                label_column_name=\"Quantity\",\n",
        "                                n_cross_validations=5,\n",
        "                                **time_series_settings\n",
        "                        )\n",
        "\n",
        "local_run = experiment.submit(automl_config, show_output = False)\n",
        "best_run, fitted_model = local_run.get_output()\n",
        "\n",
        "with open(model_name, mode='wb') as file:\n",
        "   joblib.dump(fitted_model, file)\n",
        "\n",
        "model = Model.register(workspace=ws, model_name=model_name, model_path=model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T17:07:30.8731168Z",
              "execution_start_time": "2021-11-05T17:07:27.0139739Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T17:07:26.9258814Z",
              "session_id": 5,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 13
            },
            "text/plain": [
              "StatementMeta(spark001, 5, 13, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registering model 1066_tropicana"
          ]
        }
      ],
      "source": [
        "best_run, fitted_model = local_run.get_output()\n",
        "\n",
        "with open(model_name, mode='wb') as file:\n",
        "   joblib.dump(fitted_model, file)\n",
        "\n",
        "model = Model.register(workspace=ws, model_name=model_name, model_path=model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Scale it up with many model training with function Pandas API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:55:21.0514794Z",
              "execution_start_time": "2021-11-05T19:55:20.560184Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:55:20.4608678Z",
              "session_id": 8,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 21
            },
            "text/plain": [
              "StatementMeta(spark001, 8, 21, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Prepare the core training function\n",
        "\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Model\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "import pandas as pd\n",
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "import cloudpickle\n",
        "#do not use joblib to dump because it will have issue with multi-level object\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "\n",
        "schema = StructType([ \\\n",
        "    StructField(\"Store\",StringType(),True), \\\n",
        "    StructField(\"Brand\",StringType(),True), \\\n",
        "\n",
        "  ])\n",
        "\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def many_model_train(train_data_df):\n",
        "        sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n",
        "                                                service_principal_id=service_principal_id,\n",
        "                                                service_principal_password=service_principal_password)\n",
        "        # Instantiate Azure Machine Learning workspace\n",
        "        ws = Workspace.get(name=workspace_name,\n",
        "                        subscription_id=subscription_id,\n",
        "                        resource_group=resource_group,auth= sp_auth)\n",
        "        experiment_name = 'automl-ml-forecast-local'\n",
        "\n",
        "        experiment=Experiment(ws, experiment_name)\n",
        "\n",
        "        target_column= 'Quantity'\n",
        "        timestamp_column= 'WeekStarting'\n",
        "        timeseries_id_columns= [ 'Store', 'Brand']\n",
        "        drop_columns=['Revenue', 'Store', 'Brand']\n",
        "        model_type= 'lr'\n",
        "        #Get the store and brand. They are unique from the group so just the first value is sufficient\n",
        "        store = str(train_data_df['Store'][0])\n",
        "        brand = str(train_data_df['Brand'][0])\n",
        "        test_size=20\n",
        "\n",
        "        train = train_data_df[:-test_size]\n",
        "        test = train_data_df[-test_size:]\n",
        "\n",
        "        model_name=store+\"_\"+brand\n",
        "        test_size=20\n",
        "        # 1.0 Format the input data from group by, put the time in the index\n",
        "        time_column_name='WeekStarting'\n",
        "        time_series_id_column_names=['Store', 'Brand']\n",
        "        time_series_settings = {\n",
        "                                'time_column_name': time_column_name,\n",
        "                                'time_series_id_column_names': time_series_id_column_names,\n",
        "                                'forecast_horizon': 2\n",
        "                                }\n",
        "\n",
        "        automl_config = AutoMLConfig(\n",
        "                                        task = 'forecasting',\n",
        "                                        debug_log='automl_oj_sales_errors.log',\n",
        "                                        primary_metric='normalized_root_mean_squared_error',\n",
        "                                        experiment_timeout_minutes=20,\n",
        "                                        iterations=2,\n",
        "                                        training_data=train,\n",
        "                                        label_column_name=\"Quantity\",\n",
        "                                        n_cross_validations=2,\n",
        "                                        **time_series_settings\n",
        "                                )\n",
        "\n",
        "        local_run = experiment.submit(automl_config, show_output = False)\n",
        "        best_run, fitted_model = local_run.get_output()\n",
        "\n",
        "        with open(model_name, mode='wb') as file:\n",
        "                cloudpickle.dump(fitted_model, file)\n",
        "  \n",
        "        model = Model.register(workspace=ws, model_name=model_name, model_path=model_name)\n",
        "\n",
        "        return pd.DataFrame({'Store':[store],'Brand':[brand]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:55:23.1450106Z",
              "execution_start_time": "2021-11-05T19:55:22.9947712Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:55:22.9099345Z",
              "session_id": 8,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "finished",
              "statement_id": 22
            },
            "text/plain": [
              "StatementMeta(spark001, 8, 22, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n",
        "result = df.groupby([\"Brand\",\"Store\"]).apply(many_model_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": "2021-11-05T19:55:25.0867507Z",
              "livy_statement_state": "running",
              "queued_time": "2021-11-05T19:55:25.0012647Z",
              "session_id": 8,
              "session_start_time": null,
              "spark_pool": "spark001",
              "state": "submitted",
              "statement_id": 23
            },
            "text/plain": [
              "StatementMeta(spark001, 8, 23, Submitted, Running)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(result.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###Many Model Inferencing: Can you score using multiple models in parallel?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Home work: please prepare a function pandas UDF to produce forecast for mutliple store and brand given the test data"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
