{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Synapse Spark 3.x to train multiple models and perform scalable inferencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.Preperation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment preperation\n",
        "1. Prepare a Synapse Spark Pool 3.x Medium\n",
        "2. Prepare a Azure ML workspace \n",
        "3. Prepare a service principal with secret key registered in keyvault. The service principal should have contributor access to your Azure ML workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load util classes and models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:53:04.0925508Z",
              "execution_start_time": "2021-11-05T19:53:00.1456486Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:50:31.7755597Z",
              "session_id": 4,
              "session_start_time": "2021-11-05T19:50:31.8049509Z",
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(spark31, 4, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "class ColumnDropper(TransformerMixin, BaseEstimator):\n",
        "    \"\"\"\n",
        "    Transformer for dropping columns from a dataframe.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_columns):\n",
        "        assert isinstance(drop_columns, list), \"Expected drop_columns input to be a list\"\n",
        "        self.drop_columns = drop_columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.drop(columns=self.drop_columns, errors='ignore')\n",
        "\n",
        "\n",
        "class SimpleCalendarFeaturizer(TransformerMixin, BaseEstimator):\n",
        "    \"\"\"\n",
        "    Transformer for adding a simple calendar feature derived from the input time index.\n",
        "    For demonstration purposes, the transform creates a feature for week of the year.\n",
        "    \"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.assign(Week_Year=X.index.isocalendar().week.values)\n",
        "\n",
        "\n",
        "class SimpleLagger(TransformerMixin, BaseEstimator):\n",
        "    \"\"\"\n",
        "    Simple lagging transform that creates lagged values of the target column.\n",
        "    This transform uses information known at fit time to create lags at transform time\n",
        "    to maintain lag feature continuity across train/test splits.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_column_name, lag_orders=None):\n",
        "        my_lag_orders = lag_orders if lag_orders is not None else [1]\n",
        "        assert isinstance(my_lag_orders, list) and min(my_lag_orders) > 0, \\\n",
        "            'Expected lag_orders to be a list of integers all greater than zero'\n",
        "        self.target_column_name = target_column_name\n",
        "        self.lag_orders = my_lag_orders\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the lagger transform.\n",
        "        This transform caches the tail of the training data up to the maximum lag order\n",
        "        so that lag features can be created on test data.\n",
        "        \"\"\"\n",
        "        assert self.target_column_name in X.columns, \\\n",
        "            \"Target column is missing from the input dataframe.\"\n",
        "\n",
        "        X_fit = X.sort_index(ascending=True)\n",
        "        max_lag_order = max(self.lag_orders)\n",
        "        self._train_tail = X_fit.iloc[-max_lag_order:]\n",
        "        self._column_order = self._train_tail.columns\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Create lag features of the target for the input data.\n",
        "        The transform uses data cached at fit time, if necessary, to provide\n",
        "        continuity of lag features.\n",
        "        \"\"\"\n",
        "        X_trans = X.copy()\n",
        "        added_target = False\n",
        "        if self.target_column_name not in X_trans.columns:\n",
        "            X_trans[self.target_column_name] = np.nan\n",
        "            added_target = True\n",
        "\n",
        "        # decide if we need to use the training cache i.e. are we in a test scenario?\n",
        "        train_latest = self._train_tail.index.max()\n",
        "        X_earliest = X_trans.index.min()\n",
        "        if train_latest < X_earliest:\n",
        "            # X data is later than the training period - append the cached tail of training data\n",
        "            X_trans = pd.concat((self._train_tail, X_trans[self._column_order]))\n",
        "\n",
        "        # Ensure data is sorted by time before making lags\n",
        "        X_trans.sort_index(ascending=True, inplace=True)\n",
        "\n",
        "        # Make the lag features\n",
        "        for lag_order in self.lag_orders:\n",
        "            X_trans['lag_' + str(lag_order)] = X_trans[self.target_column_name].shift(lag_order)\n",
        "\n",
        "        # Return transformed dataframe with the same time range as X\n",
        "        if added_target:\n",
        "            X_trans.drop(columns=[self.target_column_name], inplace=True)\n",
        "        return X_trans.loc[X.index]\n",
        "\n",
        "\n",
        "class SklearnWrapper(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Wrapper class around an sklearn model.\n",
        "    This wrapper formats DataFrame input for scikit-learn regression estimators.\n",
        "    \"\"\"\n",
        "    def __init__(self, sklearn_model, target_column_name):\n",
        "        self.sklearn_model = sklearn_model\n",
        "        self.target_column_name = target_column_name\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the sklearn model on the input dataframe.\n",
        "        \"\"\"\n",
        "        assert self.target_column_name in X.columns, \\\n",
        "            \"Target column is missing from the input dataframe.\"\n",
        "\n",
        "        # Drop rows with missing values and check that we still have data left\n",
        "        X_fit = X.dropna()\n",
        "        assert len(X_fit) > 0, 'Training dataframe is empty after dropping NA values'\n",
        "\n",
        "        # Check that data is all numeric type\n",
        "        # This simple pipeline does not handle categoricals or other non-numeric types\n",
        "        full_col_set = set(X_fit.columns)\n",
        "        numeric_col_set = set(X_fit.select_dtypes(include=[np.number]).columns)\n",
        "        assert full_col_set == numeric_col_set, \\\n",
        "            ('Found non-numeric columns {} in the input dataframe. Please drop them prior to modeling.'\n",
        "             .format(full_col_set - numeric_col_set))\n",
        "\n",
        "        # Fit the scikit model\n",
        "        y_fit = X_fit.pop(self.target_column_name)\n",
        "        self._column_order = X_fit.columns\n",
        "        self.sklearn_model.fit(X_fit.values, y_fit.values)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Identity transform for fit_transform pipelines.\n",
        "        \"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict on the input dataframe.\n",
        "        Return a Pandas Series with time in the index\n",
        "        \"\"\"\n",
        "        # Check the column set in input is compatible with fitted model\n",
        "        input_col_set = set(X.columns) - set([self.target_column_name])\n",
        "        assert input_col_set == set(self._column_order), \\\n",
        "            'Input columns {} do not match expected columns {}'.format(input_col_set, self._column_order)\n",
        "\n",
        "        X_pred = X.drop(columns=[self.target_column_name], errors='ignore')[self._column_order]\n",
        "        X_pred.dropna(inplace=True)\n",
        "        assert len(X_pred) > 0, 'Prediction dataframe is empty after dropping NA values'\n",
        "        y_raw = self.sklearn_model.predict(X_pred.values)\n",
        "        return pd.Series(data=y_raw, index=X_pred.index)\n",
        "\n",
        "\n",
        "class SimpleForecaster(TransformerMixin):\n",
        "    \"\"\"\n",
        "    Forecasting class for a simple, 1-step ahead forecaster.\n",
        "    This class encapsulates fitting a transform pipeline with an sklearn regression estimator\n",
        "    and producing in-sample and out-of-sample forecasts.\n",
        "    Out-of-sample forecasts apply the model recursively over the prediction set to produce forecasts\n",
        "    at any horizon.\n",
        "    The forecaster assumes that the time-series data is regularly sampled on a contiguous interval;\n",
        "    it does not handle missing values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transform_steps, estimator, target_column_name, time_column_name):\n",
        "        assert estimator is not None, \"Estimator cannot be None.\"\n",
        "        assert transform_steps is None or isinstance(transform_steps, list), \\\n",
        "            \"transform_steps should be a list\"\n",
        "        estimator_step = ('estimator', SklearnWrapper(estimator, target_column_name))\n",
        "        steps = transform_steps + [estimator_step] if transform_steps is not None else [estimator_step]\n",
        "        self.pipeline = Pipeline(steps=steps)\n",
        "\n",
        "        self.target_column_name = target_column_name\n",
        "        self.time_column_name = time_column_name\n",
        "\n",
        "    def _recursive_forecast(self, X):\n",
        "        \"\"\"\n",
        "        Apply the trained model resursively for out-of-sample predictions.\n",
        "        \"\"\"\n",
        "        X_fcst = X.sort_index(ascending=True)\n",
        "        if self.target_column_name not in X_fcst.columns:\n",
        "            X_fcst[self.target_column_name] = np.nan\n",
        "\n",
        "        forecasts = pd.Series(np.nan, index=X_fcst.index)\n",
        "        for fcst_date in X_fcst.index.get_level_values(self.time_column_name):\n",
        "            # Get predictions on an expanding window ending on the current forecast date\n",
        "            y_fcst = self.pipeline.predict(X_fcst[X_fcst.index <= fcst_date])\n",
        "\n",
        "            # Set the current forecast\n",
        "            forecasts.loc[fcst_date] = y_fcst.loc[fcst_date]\n",
        "\n",
        "            # Set the actual value to the forecast value so that lag features can be made on next iteration\n",
        "            X_fcst.loc[fcst_date, self.target_column_name] = y_fcst.loc[fcst_date]\n",
        "\n",
        "        return forecasts\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Fit the forecasting pipeline.\n",
        "        This method assumes the target is a column in the input, X.\n",
        "        \"\"\"\n",
        "        assert list(X.index.names) == [self.time_column_name], \\\n",
        "            \"Expected time column to comprise input dataframe index.\"\n",
        "        self._latest_training_date = X.index.max()\n",
        "        self.pipeline.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the data through the pipeline.\n",
        "        \"\"\"\n",
        "        return self.pipeline.transform(X)\n",
        "\n",
        "    def forecast(self, X):\n",
        "        \"\"\"\n",
        "        Make forecasts over the prediction frame, X.\n",
        "        X can contain in-sample and out-of-sample data.\n",
        "        For out-of-sample data, the 1-step-ahead model is recursively applied.\n",
        "        Returns forecasts for the target in a pd.Series object with the same time index as X.\n",
        "        np.nan values will be returned for dates where a forecast could not be found.\n",
        "        \"\"\"\n",
        "        assert list(X.index.names) == [self.time_column_name], \\\n",
        "            \"Expected time column to comprise input dataframe index.\"\n",
        "        # Get in-sample forecasts if requested\n",
        "        X_insamp = X[X.index <= self._latest_training_date]\n",
        "        forecasts_insamp = pd.Series()\n",
        "        if len(X_insamp) > 0:\n",
        "            forecasts_insamp = self.pipeline.predict(X_insamp)\n",
        "\n",
        "        # Get out-of-sample forecasts\n",
        "        X_fcst = X[X.index > self._latest_training_date]\n",
        "        forecasts = pd.Series()\n",
        "        if len(X_fcst) > 0:\n",
        "            # Need to iterate/recurse 1-step forecasts here\n",
        "            forecasts = self._recursive_forecast(X_fcst)\n",
        "        forecasts = pd.concat((forecasts_insamp, forecasts))\n",
        "\n",
        "        return forecasts.reindex(X.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://azure.microsoft.com/en-us/services/open-datasets/catalog/sample-oj-sales-simulated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:04:48.1830923Z",
              "execution_start_time": "2021-11-05T14:04:29.3452697Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:01:59.0238115Z",
              "session_id": 1,
              "session_start_time": "2021-11-05T14:01:59.0479754Z",
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(spark31, 1, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data =spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://ojsales-simulatedcontainer@azureopendatastorage.blob.core.windows.net/oj_sales_data/Store10*.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T19:53:04.3339492Z",
              "execution_start_time": "2021-11-05T19:53:04.1829885Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T19:50:42.9340966Z",
              "session_id": 4,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(spark31, 4, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error",
          "traceback": [
            "NameError: name 'data' is not defined",
            "Traceback (most recent call last):\n",
            "NameError: name 'data' is not defined\n"
          ]
        }
      ],
      "source": [
        "#Write to local delta for fast reading\n",
        "data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"OJ_Sales_Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "microsoft": {
          "language": "sparksql"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:09:12.2050607Z",
              "execution_start_time": "2021-11-05T14:08:43.2403679Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:08:43.1532051Z",
              "session_id": 1,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(spark31, 1, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [
                [
                  "1990-06-14",
                  "1097",
                  "minute.maid",
                  "9626",
                  "1",
                  "2.45",
                  "23583.7"
                ],
                [
                  "1990-06-21",
                  "1097",
                  "minute.maid",
                  "18695",
                  "1",
                  "2.31",
                  "43185.450000000004"
                ],
                [
                  "1990-06-28",
                  "1097",
                  "minute.maid",
                  "13535",
                  "1",
                  "2.1",
                  "28423.5"
                ],
                [
                  "1990-07-05",
                  "1097",
                  "minute.maid",
                  "17289",
                  "1",
                  "2.46",
                  "42530.94"
                ],
                [
                  "1990-07-12",
                  "1097",
                  "minute.maid",
                  "16015",
                  "1",
                  "1.99",
                  "31869.85"
                ],
                [
                  "1990-07-19",
                  "1097",
                  "minute.maid",
                  "13643",
                  "1",
                  "2.55",
                  "34789.649999999994"
                ],
                [
                  "1990-07-26",
                  "1097",
                  "minute.maid",
                  "15754",
                  "1",
                  "1.9",
                  "29932.6"
                ],
                [
                  "1990-08-02",
                  "1097",
                  "minute.maid",
                  "9762",
                  "1",
                  "1.9",
                  "18547.8"
                ],
                [
                  "1990-08-09",
                  "1097",
                  "minute.maid",
                  "11676",
                  "1",
                  "2.28",
                  "26621.28"
                ],
                [
                  "1990-08-16",
                  "1097",
                  "minute.maid",
                  "17712",
                  "1",
                  "2.41",
                  "42685.920000000006"
                ]
              ],
              "schema": {
                "fields": [
                  {
                    "metadata": {},
                    "name": "WeekStarting",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Store",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Brand",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Quantity",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Advert",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Price",
                    "nullable": true,
                    "type": "string"
                  },
                  {
                    "metadata": {},
                    "name": "Revenue",
                    "nullable": true,
                    "type": "string"
                  }
                ],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 10 rows and 7 fields>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql \n",
        "select * from OJ_Sales_Data limit 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .table-result-container {\n",
              "    max-height: 300px;\n",
              "    overflow: auto;\n",
              "  }\n",
              "  table, th, td {\n",
              "    border: 1px solid black;\n",
              "    border-collapse: collapse;\n",
              "  }\n",
              "  th, td {\n",
              "    padding: 5px;\n",
              "  }\n",
              "  th {\n",
              "    text-align: left;\n",
              "  }\n",
              "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(DISTINCT store, brand)</th></tr></thead><tbody><tr><td>300</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%sql select count (distinct store, brand) from OJ_Sales_Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .table-result-container {\n",
              "    max-height: 300px;\n",
              "    overflow: auto;\n",
              "  }\n",
              "  table, th, td {\n",
              "    border: 1px solid black;\n",
              "    border-collapse: collapse;\n",
              "  }\n",
              "  th, td {\n",
              "    padding: 5px;\n",
              "  }\n",
              "  th {\n",
              "    text-align: left;\n",
              "  }\n",
              "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>brand</th></tr></thead><tbody><tr><td>dominicks</td></tr><tr><td>tropicana</td></tr><tr><td>minute.maid</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%sql select distinct brand from OJ_Sales_Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Warm-up exersize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Read about Pandas Function APIs: https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/pandas-function-apis\n",
        "2. Answer following questions:\n",
        "  - What is the advantage of this technology vs. regular Python UDF?\n",
        "  - What is the role of Apache Arrow in this?\n",
        "  - What is the use of iterator and yield vs. regular list and return?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the OJ sales dataset above, use Pandas Function APIs, pick out for each store and brand the best selling week in the form of week_number-yyyy.\n",
        "The result set look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:09:15.0097704Z",
              "execution_start_time": "2021-11-05T14:09:12.281749Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:09:03.1835606Z",
              "session_id": 1,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(spark31, 1, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "a62d323b-99f4-4ba8-b6b8-140fa393b8e0",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, a62d323b-99f4-4ba8-b6b8-140fa393b8e0)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "result_sample= pd.DataFrame({\"store\": [1066, 1067, 1068],'Brand':['dominicks', 'tropicana','tropicana'],\"Best_Selling_Week\": ['23-1992', '24-1991','24-1991']})\n",
        "display(result_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:09:37.938814Z",
              "execution_start_time": "2021-11-05T14:09:15.1012729Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:09:04.4474532Z",
              "session_id": 1,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(spark31, 1, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "f917d030-68aa-4170-b080-c81bcc3c7c82",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, f917d030-68aa-4170-b080-c81bcc3c7c82)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Solution\n",
        "#The Pandas function\n",
        "import pandas as pd\n",
        "def best_selling_week(inputdf):\n",
        "  store =inputdf['Store'][0]\n",
        "  brand = inputdf['Brand'][0]\n",
        "  best_week_row = inputdf.iloc[inputdf['Quantity'].argmax()]\n",
        "  best_week =str(best_week_row['WeekStarting'].isocalendar()[1]) +\"-\"+ str(best_week_row['WeekStarting'].isocalendar()[0])\n",
        "  qty = best_week_row['Quantity']\n",
        "\n",
        "  return pd.DataFrame({\"Store\":[store], \"Brand\":[brand], \"Best_Selling_Week\":best_week, \"Qty\":[qty]})\n",
        "  \n",
        "\n",
        "df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n",
        "df = df.repartition(200) #to increase parallelism\n",
        "#Use the pandas function in group by\n",
        "\n",
        "result = df.groupby([\"Brand\",\"Store\"]).applyInPandas(best_selling_week, schema=\"Store string, Brand string, Best_Selling_Week string, Qty float\")\n",
        "display(result.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Useful knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Map function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You perform map operations with pandas instances by DataFrame.mapInPandas() in order to transform an iterator of pandas.DataFrame to another iterator of pandas.DataFrame that represents the current PySpark DataFrame and returns the result as a PySpark DataFrame.\n",
        "\n",
        "The underlying function takes and outputs an iterator of pandas.DataFrame. It can return the output of arbitrary length in contrast to some pandas UDFs such as Series to Series pandas UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:33:27.9703597Z",
              "execution_start_time": "2021-11-05T14:33:27.8269506Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:33:27.7414891Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "spark.conf.set(' spark.sql.execution.arrow.maxRecordsPerBatch', 100)\n",
        "#Default is 10000 which in some cases may defeat the purpose of parallelism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:34:20.0505693Z",
              "execution_start_time": "2021-11-05T14:33:28.637346Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:33:28.5538209Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "21fbd9d0-3984-4d57-92cf-93cc1dbeb7ea",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 21fbd9d0-3984-4d57-92cf-93cc1dbeb7ea)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def parallel_transform(df_iterator):\n",
        "  for df in df_iterator:\n",
        "    df['Week'] = df['WeekStarting'].map(lambda x: str(x.isocalendar()[1]) +\"-\"+ str(x.isocalendar()[0]))\n",
        "    df.drop(\"WeekStarting\", inplace=True, axis=1)\n",
        "    yield df\n",
        "df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n",
        "\n",
        "result = df.mapInPandas(parallel_transform, schema=\"Store string, Brand string, Week string, Quantity float, Revenue string\")\n",
        "\n",
        "display(result.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Many Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:34:20.3095166Z",
              "execution_start_time": "2021-11-05T14:34:20.1510792Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:33:30.6587184Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#prepare values to broadcast\n",
        "tenant_id ='' \n",
        "service_principal_id=''\n",
        "service_principal_password=''\n",
        "subscription_id = ''\n",
        "# Azure Machine Learning resource group NOT the managed resource group\n",
        "resource_group = '' \n",
        "\n",
        "#Azure Machine Learning workspace name, NOT Azure Databricks workspace\n",
        "workspace_name = ''  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test with a single store & brand combination (single time series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./timeseries_utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:35:32.9896035Z",
              "execution_start_time": "2021-11-05T14:35:24.4800745Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:35:24.366107Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Getting data\n",
        "import pandas as pd\n",
        "train_data_df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data where Store = '1066' and Brand ='tropicana'\").toPandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:09:55.0342126Z",
              "execution_start_time": "2021-11-05T14:09:54.5488232Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:09:54.4672919Z",
              "session_id": 1,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 10
            },
            "text/plain": [
              "StatementMeta(spark31, 1, 10, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "c660445e-5186-42a7-ab1e-a42e7cfaca4d",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, c660445e-5186-42a7-ab1e-a42e7cfaca4d)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(train_data_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:35:34.1201781Z",
              "execution_start_time": "2021-11-05T14:35:33.0752342Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:35:29.2898789Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Featurized data example:\n",
            "\n",
            "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:289: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
            "  Unsupported type in conversion from Arrow: uint32\n",
            "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
            "  warnings.warn(msg)"
          ]
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "7b9064d4-0adb-4162-a978-f60bec951379",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 7b9064d4-0adb-4162-a978-f60bec951379)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "#Getting data for one table to test the utility function\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import joblib\n",
        "import os\n",
        "target_column= 'Quantity'\n",
        "timestamp_column= 'WeekStarting'\n",
        "timeseries_id_columns= [ 'Store', 'Brand']\n",
        "drop_columns=['Revenue', 'Store', 'Brand']\n",
        "model_type= 'lr'\n",
        "model_name=train_data_df['Store'][0]+\"_\"+train_data_df['Brand'][0]\n",
        "test_size=20\n",
        "# 1.0 Read the data from CSV - parse timestamps as datetime type and put the time in the index\n",
        "data = train_data_df \\\n",
        "        .set_index('WeekStarting') \\\n",
        "        .sort_index(ascending=True)\n",
        "\n",
        "# 2.0 Split the data into train and test sets\n",
        "train = data[:-test_size]\n",
        "test = data[-test_size:]\n",
        "\n",
        "# 3.0 Create and fit the forecasting pipeline\n",
        "# The pipeline will drop unhelpful features, make a calendar feature, and make lag features\n",
        "lagger = SimpleLagger(target_column, lag_orders=[1, 2, 3, 4])\n",
        "transform_steps = [('column_dropper', ColumnDropper(drop_columns)),\n",
        "                   ('calendar_featurizer', SimpleCalendarFeaturizer()), ('lagger', lagger)]\n",
        "forecaster = SimpleForecaster(transform_steps, LinearRegression(), target_column, timestamp_column)\n",
        "forecaster.fit(train)\n",
        "print('Featurized data example:')\n",
        "display(forecaster.transform(train).head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:38:52.9610643Z",
              "execution_start_time": "2021-11-05T14:38:49.0751159Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T14:38:48.9644989Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 13
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 13, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registering model 1066_tropicana\n",
            "<stdin>:229: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "<stdin>:235: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning."
          ]
        }
      ],
      "source": [
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Model\n",
        "\n",
        "import cloudpickle \n",
        "import joblib\n",
        "sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n",
        "                                         service_principal_id=service_principal_id,\n",
        "                                         service_principal_password=service_principal_password)\n",
        "# Instantiate Azure Machine Learning workspace\n",
        "ws = Workspace.get(name=workspace_name,\n",
        "                   subscription_id=subscription_id,\n",
        "                   resource_group=resource_group,auth= sp_auth)\n",
        "\n",
        "\n",
        "# 4.0 Get predictions on test set\n",
        "forecasts = forecaster.forecast(test)\n",
        "compare_data = test.assign(forecasts=forecasts).dropna()\n",
        "\n",
        "# 5.0 Calculate accuracy metrics for the fit\n",
        "mse = mean_squared_error(compare_data[target_column], compare_data['forecasts'])\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(compare_data[target_column], compare_data['forecasts'])\n",
        "actuals = compare_data[target_column].values\n",
        "preds = compare_data['forecasts'].values\n",
        "mape = np.mean(np.abs((actuals - preds) / actuals) * 100)\n",
        "\n",
        "# 7.0 Train model with full dataset\n",
        "forecaster.fit(data)\n",
        "\n",
        "# 8.0 Save the forecasting pipeline\n",
        "with open(model_name, mode='wb') as file:\n",
        "   joblib.dump(forecaster, file)\n",
        "\n",
        "model = Model.register(workspace=ws, model_name=model_name, model_path=model_name, tags={'mse':str(mse), 'mape': str(mape), 'rmse': str(rmse)})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####Scale it up with many model training with function Pandas API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:41:13.7704049Z",
              "execution_start_time": null,
              "livy_statement_state": null,
              "queued_time": "2021-11-05T14:41:05.5518525Z",
              "session_id": null,
              "session_start_time": null,
              "spark_pool": null,
              "state": "cancelled",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Cancelled, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Prepare the core training function\n",
        "\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Model\n",
        "import cloudpickle\n",
        "#do not use joblib to dump because it will have issue with multi-level object\n",
        "def many_model_train(train_data_df):\n",
        "  sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n",
        "                                         service_principal_id=service_principal_id,\n",
        "                                         service_principal_password=service_principal_password)\n",
        "  # Instantiate Azure Machine Learning workspace\n",
        "  ws = Workspace.get(name=workspace_name,\n",
        "                     subscription_id=subscription_id,\n",
        "                     resource_group=resource_group,auth= sp_auth)\n",
        "\n",
        "\n",
        "  target_column= 'Quantity'\n",
        "  timestamp_column= 'WeekStarting'\n",
        "  timeseries_id_columns= [ 'Store', 'Brand']\n",
        "  drop_columns=['Revenue', 'Store', 'Brand']\n",
        "  model_type= 'lr'\n",
        "  #Get the store and brand. They are unique from the group so just the first value is sufficient\n",
        "  store = train_data_df['Store'][0]\n",
        "  brand = train_data_df['Brand'][0]\n",
        "\n",
        "  model_name=store+\"_\"+brand\n",
        "  test_size=20\n",
        "  # 1.0 Format the input data from group by, put the time in the index\n",
        "  data = train_data_df \\\n",
        "          .set_index('WeekStarting') \\\n",
        "          .sort_index(ascending=True)\n",
        "\n",
        "  # 2.0 Split the data into train and test sets\n",
        "  train = data[:-test_size]\n",
        "  test = data[-test_size:]\n",
        "\n",
        "  # 3.0 Create and fit the forecasting pipeline\n",
        "  # The pipeline will drop unhelpful features, make a calendar feature, and make lag features\n",
        "  lagger = SimpleLagger(target_column, lag_orders=[1, 2, 3, 4])\n",
        "  transform_steps = [('column_dropper', ColumnDropper(drop_columns)),\n",
        "                     ('calendar_featurizer', SimpleCalendarFeaturizer()), ('lagger', lagger)]\n",
        "  forecaster = SimpleForecaster(transform_steps, LinearRegression(), target_column, timestamp_column)\n",
        "  forecaster.fit(train)\n",
        "\n",
        "  # 4.0 Get predictions on test set\n",
        "  forecasts = forecaster.forecast(test)\n",
        "  compare_data = test.assign(forecasts=forecasts).dropna()\n",
        "\n",
        "  # 5.0 Calculate accuracy metrics for the fit\n",
        "  mse = mean_squared_error(compare_data[target_column], compare_data['forecasts'])\n",
        "  rmse = np.sqrt(mse)\n",
        "  mae = mean_absolute_error(compare_data[target_column], compare_data['forecasts'])\n",
        "  actuals = compare_data[target_column].values\n",
        "  preds = compare_data['forecasts'].values\n",
        "  mape = np.mean(np.abs((actuals - preds) / actuals) * 100)\n",
        "\n",
        "  # 7.0 Train model with full dataset\n",
        "  forecaster.fit(data)\n",
        "\n",
        "  # 8.0 Save the pipeline and register model to AML\n",
        "  with open(model_name, mode='wb') as file:\n",
        "     cloudpickle.dump(forecaster, file)#   \n",
        "  model = Model.register(workspace=ws, model_name=model_name, model_path=model_name, tags={'mse':str(mse), 'mape': str(mape), 'rmse': str(rmse)})\n",
        "  \n",
        "  return pd.DataFrame({'Store':store,'Brand':brand, 'mse':[mse], 'mape': [mape], 'rmse': [rmse], 'model_name':[model_name]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T14:41:13.7712024Z",
              "execution_start_time": null,
              "livy_statement_state": null,
              "queued_time": "2021-11-05T14:41:06.3337549Z",
              "session_id": null,
              "session_start_time": null,
              "spark_pool": null,
              "state": "cancelled",
              "statement_id": null
            },
            "text/plain": [
              "StatementMeta(, , , Cancelled, )"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n",
        "df = df.repartition(200) #to increase parallelism\n",
        "result = df.groupby([\"Brand\",\"Store\"]).applyInPandas(many_model_train, schema=\"Store string, Brand string, mse float, mape float, rmse float, model_name string \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Many Model Inferencing: Can you score using multiple models in parallel?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exersize : please prepare a function pandas UDF to produce forecast for mutliple store and brand given the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quick test the forecast function in utils with just one time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T15:03:36.3226134Z",
              "execution_start_time": "2021-11-05T15:03:35.2721913Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T15:03:35.0724469Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 18
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 18, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "5f0fbd4c-1d80-4598-858a-1f7dbd58e536",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 5f0fbd4c-1d80-4598-858a-1f7dbd58e536)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "<stdin>:229: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "<stdin>:235: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning."
          ]
        }
      ],
      "source": [
        "#Test forecast for one time series, need to run command 27-30 first\n",
        "ts_id_dict = {id_col: str(data[id_col].iloc[0]) for id_col in timeseries_id_columns}\n",
        "forecasts=forecaster.forecast(data)\n",
        "prediction_df = forecasts.to_frame(name='Prediction')\n",
        "prediction_df =prediction_df.reset_index().assign(**ts_id_dict)\n",
        "display(prediction_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Main solution using map in pandas & loading models from AML workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T15:03:39.0929445Z",
              "execution_start_time": "2021-11-05T15:03:38.9298741Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T15:03:38.8473503Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 19
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 19, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Prepare the core forecast function in pandas function API  \n",
        "\n",
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Model\n",
        "import cloudpickle\n",
        "#do not use joblib to dump because it will have issue with multi-level object\n",
        "def many_model_forecast(input_data_df):\n",
        "  sp_auth = ServicePrincipalAuthentication(tenant_id =tenant_id,\n",
        "                                         service_principal_id=service_principal_id,\n",
        "                                         service_principal_password=service_principal_password)\n",
        "  # Instantiate Azure Machine Learning workspace\n",
        "  ws = Workspace.get(name=workspace_name,\n",
        "                     subscription_id=subscription_id,\n",
        "                     resource_group=resource_group,auth= sp_auth)\n",
        "\n",
        "\n",
        "  target_column= 'Quantity'\n",
        "  timestamp_column= 'WeekStarting'\n",
        "  timeseries_id_columns= [ 'Store', 'Brand']\n",
        "  drop_columns=['Revenue', 'Store', 'Brand']\n",
        "  data = input_data_df \\\n",
        "        .set_index(timestamp_column) \\\n",
        "        .sort_index(ascending=True)\n",
        "  #Prepare loading model from Azure ML, get the latest model by default\n",
        "  model_name=data['Store'][0]+\"_\"+data['Brand'][0]\n",
        "  model = Model(ws, model_name)\n",
        "  model.download(exist_ok =True)\n",
        "  with open(model_name, 'rb') as f:\n",
        "    forecaster = cloudpickle.load(f)\n",
        "\n",
        "#   Get predictions \n",
        "  #This is to append the store and brand column to the result\n",
        "  ts_id_dict = {id_col: str(data[id_col].iloc[0]) for id_col in timeseries_id_columns}\n",
        "  forecasts=forecaster.forecast(data)\n",
        "  prediction_df = forecasts.to_frame(name='Prediction')\n",
        "  prediction_df =prediction_df.reset_index().assign(**ts_id_dict)\n",
        "  \n",
        "  return prediction_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T15:03:41.670222Z",
              "execution_start_time": "2021-11-05T15:03:41.1562377Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T15:03:41.0653151Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 20
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 20, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Load data to score, for now, it's same train data but in reality, it should be different.\n",
        "df = spark.sql(\"select to_timestamp(WeekStarting) WeekStarting, float(Quantity), Brand,Revenue, Store from OJ_Sales_Data\")\n",
        "df = df.repartition(200) #to increase parallelism\n",
        "prediction_result = df.groupby([\"Brand\",\"Store\"]).applyInPandas(many_model_forecast, schema=\"WeekStarting date, Store string, Brand string, Prediction float\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-11-05T15:03:49.753368Z",
              "execution_start_time": "2021-11-05T15:03:42.8991144Z",
              "livy_statement_state": "available",
              "queued_time": "2021-11-05T15:03:42.8196171Z",
              "session_id": 2,
              "session_start_time": null,
              "spark_pool": "spark31",
              "state": "finished",
              "statement_id": 21
            },
            "text/plain": [
              "StatementMeta(spark31, 2, 21, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "c9c325ea-4958-46a3-b170-a16000a7416f",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, c9c325ea-4958-46a3-b170-a16000a7416f)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(prediction_result.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Small ask: can you add a actual qty column to the result if the data to score has it?"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "21fbd9d0-3984-4d57-92cf-93cc1dbeb7ea": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "3"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "24-1990",
                  "3": "9626.0",
                  "4": "23583.7"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "25-1990",
                  "3": "18695.0",
                  "4": "43185.450000000004"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "26-1990",
                  "3": "13535.0",
                  "4": "28423.5"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "27-1990",
                  "3": "17289.0",
                  "4": "42530.94"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "28-1990",
                  "3": "16015.0",
                  "4": "31869.85"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "29-1990",
                  "3": "13643.0",
                  "4": "34789.649999999994"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "30-1990",
                  "3": "15754.0",
                  "4": "29932.6"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "31-1990",
                  "3": "9762.0",
                  "4": "18547.8"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "32-1990",
                  "3": "11676.0",
                  "4": "26621.28"
                },
                {
                  "0": "1097",
                  "1": "minute.maid",
                  "2": "33-1990",
                  "3": "17712.0",
                  "4": "42685.920000000006"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "Store",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "Brand",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "Week",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "Quantity",
                  "type": "double"
                },
                {
                  "key": "4",
                  "name": "Revenue",
                  "type": "string"
                }
              ]
            }
          },
          "type": "Synapse.DataFrame"
        },
        "5f0fbd4c-1d80-4598-858a-1f7dbd58e536": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "1"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "1990-06-14 00:00:00",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-06-21 00:00:00",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-06-28 00:00:00",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-07-05 00:00:00",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-07-12 00:00:00",
                  "1": "14410.43605810535",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-07-19 00:00:00",
                  "1": "14569.927951313915",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-07-26 00:00:00",
                  "1": "14580.116685346551",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-08-02 00:00:00",
                  "1": "14873.03064811736",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-08-09 00:00:00",
                  "1": "15168.468388925961",
                  "2": "1066",
                  "3": "tropicana"
                },
                {
                  "0": "1990-08-16 00:00:00",
                  "1": "14837.636937709074",
                  "2": "1066",
                  "3": "tropicana"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "WeekStarting",
                  "type": "timestamp"
                },
                {
                  "key": "1",
                  "name": "Prediction",
                  "type": "double"
                },
                {
                  "key": "2",
                  "name": "Store",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "Brand",
                  "type": "string"
                }
              ]
            }
          },
          "type": "Synapse.DataFrame"
        },
        "7b9064d4-0adb-4162-a978-f60bec951379": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "1"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "13198.0",
                  "1": "24",
                  "2": "NaN",
                  "3": "NaN",
                  "4": "NaN",
                  "5": "NaN"
                },
                {
                  "0": "12188.0",
                  "1": "25",
                  "2": "13198.0",
                  "3": "NaN",
                  "4": "NaN",
                  "5": "NaN"
                },
                {
                  "0": "10453.0",
                  "1": "26",
                  "2": "12188.0",
                  "3": "13198.0",
                  "4": "NaN",
                  "5": "NaN"
                },
                {
                  "0": "13390.0",
                  "1": "27",
                  "2": "10453.0",
                  "3": "12188.0",
                  "4": "13198.0",
                  "5": "NaN"
                },
                {
                  "0": "12798.0",
                  "1": "28",
                  "2": "13390.0",
                  "3": "10453.0",
                  "4": "12188.0",
                  "5": "13198.0"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "Quantity",
                  "type": "double"
                },
                {
                  "key": "1",
                  "name": "Week_Year",
                  "type": "bigint"
                },
                {
                  "key": "2",
                  "name": "lag_1",
                  "type": "double"
                },
                {
                  "key": "3",
                  "name": "lag_2",
                  "type": "double"
                },
                {
                  "key": "4",
                  "name": "lag_3",
                  "type": "double"
                },
                {
                  "key": "5",
                  "name": "lag_4",
                  "type": "double"
                }
              ]
            }
          },
          "type": "Synapse.DataFrame"
        },
        "a62d323b-99f4-4ba8-b6b8-140fa393b8e0": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "1"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "1066",
                  "1": "dominicks",
                  "2": "23-1992"
                },
                {
                  "0": "1067",
                  "1": "tropicana",
                  "2": "24-1991"
                },
                {
                  "0": "1068",
                  "1": "tropicana",
                  "2": "24-1991"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "store",
                  "type": "bigint"
                },
                {
                  "key": "1",
                  "name": "Brand",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "Best_Selling_Week",
                  "type": "string"
                }
              ]
            }
          },
          "type": "Synapse.DataFrame"
        },
        "c660445e-5186-42a7-ab1e-a42e7cfaca4d": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "1"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "1990-06-14 00:00:00",
                  "1": "13198.0",
                  "2": "tropicana",
                  "3": "29695.5",
                  "4": "1066"
                },
                {
                  "0": "1990-06-21 00:00:00",
                  "1": "12188.0",
                  "2": "tropicana",
                  "3": "27179.24",
                  "4": "1066"
                },
                {
                  "0": "1990-06-28 00:00:00",
                  "1": "10453.0",
                  "2": "tropicana",
                  "3": "25505.32",
                  "4": "1066"
                },
                {
                  "0": "1990-07-05 00:00:00",
                  "1": "13390.0",
                  "2": "tropicana",
                  "3": "35349.6",
                  "4": "1066"
                },
                {
                  "0": "1990-07-12 00:00:00",
                  "1": "12798.0",
                  "2": "tropicana",
                  "3": "29691.359999999997",
                  "4": "1066"
                },
                {
                  "0": "1990-07-19 00:00:00",
                  "1": "18476.0",
                  "2": "tropicana",
                  "3": "49146.16",
                  "4": "1066"
                },
                {
                  "0": "1990-07-26 00:00:00",
                  "1": "16244.0",
                  "2": "tropicana",
                  "3": "35087.04",
                  "4": "1066"
                },
                {
                  "0": "1990-08-02 00:00:00",
                  "1": "16057.0",
                  "2": "tropicana",
                  "3": "35807.11",
                  "4": "1066"
                },
                {
                  "0": "1990-08-09 00:00:00",
                  "1": "16888.0",
                  "2": "tropicana",
                  "3": "35127.04",
                  "4": "1066"
                },
                {
                  "0": "1990-08-16 00:00:00",
                  "1": "14045.0",
                  "2": "tropicana",
                  "3": "30056.300000000003",
                  "4": "1066"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "WeekStarting",
                  "type": "timestamp"
                },
                {
                  "key": "1",
                  "name": "Quantity",
                  "type": "float"
                },
                {
                  "key": "2",
                  "name": "Brand",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "Revenue",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "Store",
                  "type": "string"
                }
              ]
            }
          },
          "type": "Synapse.DataFrame"
        },
        "c9c325ea-4958-46a3-b170-a16000a7416f": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "3"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "1990-06-14",
                  "1": "1031",
                  "2": "tropicana"
                },
                {
                  "0": "1990-06-21",
                  "1": "1031",
                  "2": "tropicana"
                },
                {
                  "0": "1990-06-28",
                  "1": "1031",
                  "2": "tropicana"
                },
                {
                  "0": "1990-07-05",
                  "1": "1031",
                  "2": "tropicana"
                },
                {
                  "0": "1990-07-12",
                  "1": "1031",
                  "2": "tropicana",
                  "3": "13858.3623046875"
                },
                {
                  "0": "1990-07-19",
                  "1": "1031",
                  "2": "tropicana",
                  "3": "13675.5673828125"
                },
                {
                  "0": "1990-07-26",
                  "1": "1031",
                  "2": "tropicana",
                  "3": "14244.9736328125"
                },
                {
                  "0": "1990-08-02",
                  "1": "1031",
                  "2": "tropicana",
                  "3": "14784.28515625"
                },
                {
                  "0": "1990-08-09",
                  "1": "1031",
                  "2": "tropicana",
                  "3": "14830.6640625"
                },
                {
                  "0": "1990-08-16",
                  "1": "1031",
                  "2": "tropicana",
                  "3": "14164.19140625"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "WeekStarting",
                  "type": "date"
                },
                {
                  "key": "1",
                  "name": "Store",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "Brand",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "Prediction",
                  "type": "double"
                }
              ]
            }
          },
          "type": "Synapse.DataFrame"
        },
        "f917d030-68aa-4170-b080-c81bcc3c7c82": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "3"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "1031",
                  "1": "tropicana",
                  "2": "48-1991",
                  "3": "19916.0"
                },
                {
                  "0": "1021",
                  "1": "minute.maid",
                  "2": "11-1991",
                  "3": "19947.0"
                },
                {
                  "0": "1074",
                  "1": "tropicana",
                  "2": "38-1991",
                  "3": "19932.0"
                },
                {
                  "0": "1077",
                  "1": "minute.maid",
                  "2": "15-1992",
                  "3": "19934.0"
                },
                {
                  "0": "1078",
                  "1": "minute.maid",
                  "2": "44-1991",
                  "3": "19978.0"
                },
                {
                  "0": "1019",
                  "1": "minute.maid",
                  "2": "41-1991",
                  "3": "19685.0"
                },
                {
                  "0": "1090",
                  "1": "tropicana",
                  "2": "44-1990",
                  "3": "19997.0"
                },
                {
                  "0": "1099",
                  "1": "tropicana",
                  "2": "30-1990",
                  "3": "19576.0"
                },
                {
                  "0": "1014",
                  "1": "minute.maid",
                  "2": "32-1991",
                  "3": "19995.0"
                },
                {
                  "0": "1020",
                  "1": "minute.maid",
                  "2": "43-1991",
                  "3": "19996.0"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "Store",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "Brand",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "Best_Selling_Week",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "Qty",
                  "type": "double"
                }
              ]
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
