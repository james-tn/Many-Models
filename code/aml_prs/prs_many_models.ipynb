{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-partition-per-column.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Azure Machine Learning Pipelines for Batch Inference for tabular input partitioned by column value\n",
    "\n",
    "In this notebook, we will demonstrate how to make predictions on large quantities of data asynchronously using the ML pipelines with Azure Machine Learning. Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data.\n",
    "\n",
    "> **Tip**\n",
    "If your system requires low-latency processing (to process a single document or small set of documents quickly), use [real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service) instead of batch prediction.\n",
    "\n",
    "This example will create a partitioned tabular dataset by splitting the rows in a large csv file by its value on specified column. Each partition will form up a mini-batch in the parallel processing procedure.\n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "\n",
    "- Create a tabular dataset partitioned by value on specified column.\n",
    "- Do batch inference on the dataset with each mini-batch corresponds to one partition.\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first. This sets you up with a working config file that has information on your workspace, subscription id, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janguy\\Anaconda3\\envs\\dlresearch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\janguy\\Anaconda3\\envs\\dlresearch\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\janguy\\Anaconda3\\envs\\dlresearch\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n",
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: ws01ent\n",
      "Azure region: westus2\n",
      "Subscription id: 0e9bace8-7a81-4922-83b5-d995ff706507\n",
      "Resource group: azureml\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\r\n",
    "\r\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "from azureml.core import Workspace\r\n",
    "\r\n",
    "subscription_id = '0e9bace8-7a81-4922-83b5-d995ff706507'\r\n",
    "# Azure Machine Learning resource group NOT the managed resource group\r\n",
    "resource_group = 'azureml' \r\n",
    "\r\n",
    "#Azure Machine Learning workspace name, NOT Azure Databricks workspace\r\n",
    "workspace_name = 'ws01ent'  \r\n",
    "tenant_id ='72f988bf-86f1-41af-91ab-2d7cd011db47' \r\n",
    "\r\n",
    "auth = InteractiveLoginAuthentication(tenant_id =tenant_id)\r\n",
    "# Instantiate Azure Machine Learning workspace\r\n",
    "ws = Workspace.get(name=workspace_name,\r\n",
    "                   subscription_id=subscription_id,\r\n",
    "                   resource_group=resource_group,auth= auth)\r\n",
    "\r\n",
    "print('Workspace name: ' + ws.name, \r\n",
    "      'Azure region: ' + ws.location, \r\n",
    "      'Subscription id: ' + ws.subscription_id, \r\n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\r\n",
    "\r\n",
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download OJ sales data from opendataset url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install azureml-opendatasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1000_dominicks.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1000_minute.maid.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1000_tropicana.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1001_dominicks.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1001_minute.maid.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1001_tropicana.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1002_dominicks.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1002_minute.maid.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1002_tropicana.csv',\n",
       " 'c:\\\\Users\\\\janguy\\\\OneDrive - Microsoft\\\\Documents\\\\projects\\\\training\\\\Many-Models\\\\data\\\\oj_sales_data\\\\https%3A\\\\%2Fazureopendatastorage.azurefd.net\\\\ojsales-simulatedcontainer\\\\oj_sales_data\\\\Store1003_dominicks.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\r\n",
    "from azureml.opendatasets import OjSalesSimulated\r\n",
    "dataset_maxfiles = 10 # Set to 11973 or 0 to get all the files\r\n",
    "\r\n",
    "# Pull all of the data\r\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\r\n",
    "\r\n",
    "# Pull only the first `dataset_maxfiles` files\r\n",
    "if dataset_maxfiles:\r\n",
    "    oj_sales_files = oj_sales_files.take(dataset_maxfiles)\r\n",
    "\r\n",
    "# Create a folder to download\r\n",
    "download_path = '..\\..\\data\\oj_sales_data' \r\n",
    "os.makedirs(download_path, exist_ok=True)\r\n",
    "\r\n",
    "# Download the data\r\n",
    "oj_sales_files.download(download_path, overwrite=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload OJ sales data to datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 10 files\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1000_minute.maid.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1000_minute.maid.csv, 1 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1000_tropicana.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1000_tropicana.csv, 2 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1001_dominicks.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1001_dominicks.csv, 3 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1001_minute.maid.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1001_minute.maid.csv, 4 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1001_tropicana.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1001_tropicana.csv, 5 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1002_dominicks.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1002_dominicks.csv, 6 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1002_minute.maid.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1002_minute.maid.csv, 7 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1002_tropicana.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1002_tropicana.csv, 8 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1003_dominicks.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1003_dominicks.csv, 9 files out of an estimated total of 10\n",
      "Uploading ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1000_dominicks.csv\n",
      "Uploaded ..\\..\\data\\oj_sales_data\\https%3A\\%2Fazureopendatastorage.azurefd.net\\ojsales-simulatedcontainer\\oj_sales_data\\Store1000_dominicks.csv, 10 files out of an estimated total of 10\n",
      "Uploaded 10 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_4eb0d09ea48548ea8c04115672931893"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_path = 'oj_sales_data'\r\n",
    "\r\n",
    "datastore.upload(src_dir = download_path,\r\n",
    "                target_path = target_path,\r\n",
    "                overwrite = True, \r\n",
    "                show_progress = True)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tabular dataset\n",
    "Create normal tabular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     WeekStarting  Store      Brand  Quantity  Advert  Price   Revenue\n",
      "0      1990-06-14   1000  dominicks     12003       1   2.59  31087.77\n",
      "1      1990-06-21   1000  dominicks     10239       1   2.39  24471.21\n",
      "2      1990-06-28   1000  dominicks     17917       1   2.48  44434.16\n",
      "3      1990-07-05   1000  dominicks     14218       1   2.33  33127.94\n",
      "4      1990-07-12   1000  dominicks     15925       1   2.01  32009.25\n",
      "...           ...    ...        ...       ...     ...    ...       ...\n",
      "1205   1992-09-03   1003  dominicks     10302       1   1.94  19985.88\n",
      "1206   1992-09-10   1003  dominicks     13502       1   2.16  29164.32\n",
      "1207   1992-09-17   1003  dominicks     19644       1   2.67  52449.48\n",
      "1208   1992-09-24   1003  dominicks     13860       1   2.29  31739.40\n",
      "1209   1992-10-01   1003  dominicks     11040       1   1.99  21969.60\n",
      "\n",
      "[1210 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\r\n",
    "\r\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, 'oj_sales_data/*/*/*/*/*.csv'))\r\n",
    "print(dataset.to_pandas_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the tabular dataset\n",
    "Partition the dataset by column 'store' and 'brand'. You can get a partition of data by specifying the value of one or more partition keys. E.g., by specifying `store=1000 and brand='tropicana'`, you can get all the rows that matches this condition in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Method partition_by: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Uploading file to /partition_by_key_res/de780196-0cb4-4a48-bfcd-20a1a3039fa2/\n",
      "Successfully uploaded file to datastore.\n",
      "Creating a new dataset.\n",
      "Successfully created a new dataset.\n",
      "registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Store', 'Brand']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned_dataset = dataset.partition_by(partition_keys=['Store', 'Brand'], target=(datastore, \"partition_by_key_res\"), name=\"partitioned_oj_data\")\r\n",
    "partitioned_dataset.partition_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING - Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 2)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"inferences\", datastore=datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate total revenue of each mini-batch partitioned by dataset partition key(s)\n",
    "The script sum up the total revenue of a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) Microsoft. All rights reserved.\n",
      "# Licensed under the MIT license.\n",
      "\n",
      "import os\n",
      "from azureml.core import Run, Model\n",
      "import joblib\n",
      "from util.timeseries_utilities import ColumnDropper, SimpleLagger, SimpleCalendarFeaturizer, SimpleForecaster\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
      "from sklearn.linear_model import LinearRegression\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "def init():\n",
      "    global ws\n",
      "    current_run = Run.get_context()\n",
      "    ws = current_run.experiment.workspace\n",
      "\n",
      "    print(\"Init complete\")\n",
      "\n",
      "\n",
      "def run(mini_batch):\n",
      "    print(f'run method start: {__file__}, run({mini_batch})')\n",
      "    target_column= 'Quantity'\n",
      "    timestamp_column= 'WeekStarting'\n",
      "    drop_columns=['Revenue', 'Store', 'Brand']\n",
      "    #Get the store and brand. They are unique from the group so just the first value is sufficient\n",
      "    store = str(mini_batch['Store'].iloc[0])\n",
      "    brand = str(mini_batch['Brand'].iloc[0])\n",
      "\n",
      "    model_name=\"prs_\"+store+\"_\"+brand\n",
      "    test_size=20\n",
      "    # 1.0 Format the input data from group by, put the time in the index\n",
      "    data = mini_batch \\\n",
      "            .set_index('WeekStarting') \\\n",
      "            .sort_index(ascending=True)\n",
      "\n",
      "    # 2.0 Split the data into train and test sets\n",
      "    train = data[:-test_size]\n",
      "    test = data[-test_size:]\n",
      "\n",
      "    # 3.0 Create and fit the forecasting pipeline\n",
      "    # The pipeline will drop unhelpful features, make a calendar feature, and make lag features\n",
      "    lagger = SimpleLagger(target_column, lag_orders=[1, 2, 3, 4])\n",
      "    transform_steps = [('column_dropper', ColumnDropper(drop_columns)),\n",
      "                        ('calendar_featurizer', SimpleCalendarFeaturizer()), ('lagger', lagger)]\n",
      "    forecaster = SimpleForecaster(transform_steps, LinearRegression(), target_column, timestamp_column)\n",
      "    forecaster.fit(train)\n",
      "\n",
      "    # 4.0 Get predictions on test set\n",
      "    forecasts = forecaster.forecast(test)\n",
      "    compare_data = test.assign(forecasts=forecasts).dropna()\n",
      "\n",
      "    # 5.0 Calculate accuracy metrics for the fit\n",
      "    mse = mean_squared_error(compare_data[target_column], compare_data['forecasts'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    mae = mean_absolute_error(compare_data[target_column], compare_data['forecasts'])\n",
      "    actuals = compare_data[target_column].values\n",
      "    preds = compare_data['forecasts'].values\n",
      "    mape = np.mean(np.abs((actuals - preds) / actuals) * 100)\n",
      "\n",
      "    # 7.0 Train model with full dataset\n",
      "    forecaster.fit(data)\n",
      "\n",
      "    # 8.0 Save the pipeline and register model to AML\n",
      "    joblib.dump(forecaster, model_name)#   \n",
      "    model = Model.register(workspace=ws, model_name=model_name, model_path=model_name, tags={'mse':str(mse), 'mape': str(mape), 'rmse': str(rmse)})\n",
      "    # return 1\n",
      "    result =pd.DataFrame({'Store':[store],'Brand':[brand], 'mse':[mse], 'mape': [mape], 'rmse': [rmse], 'model_name':[model_name]})\n",
      "    print(\"Result is \", result)\n",
      "    return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripts_folder = \"..\\..\\code\"\r\n",
    "inference_script_file = \"aml_prs\\prediction.py\"\r\n",
    "train_script_file = \"aml_prs\\model_train.py\"\r\n",
    "\r\n",
    "# peek at contents\r\n",
    "with open(os.path.join(scripts_folder, train_script_file)) as train_file:\r\n",
    "    print(train_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the batch inference pipeline\n",
    "### Specify the environment to run the script\n",
    "You would need to specify the required private azureml packages in dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\r\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\r\n",
    "\r\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn', 'pandas', 'joblib', 'azureml-defaults', 'azureml-core', 'azureml-dataprep[fuse]'])\r\n",
    "batch_env = Environment(name=\"many_models_environment\")\r\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\r\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Parameter partition_keys: This is an experimental parameter, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\r\n",
    "\r\n",
    "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\r\n",
    "parallel_run_train_config = ParallelRunConfig(\r\n",
    "    source_directory=scripts_folder,\r\n",
    "    entry_script=train_script_file,  # the user script to run against each input\r\n",
    "    partition_keys=['Store', 'Brand'],\r\n",
    "    error_threshold=-1,\r\n",
    "    output_action='append_row',\r\n",
    "    append_row_file_name=\"training_output.txt\",\r\n",
    "    environment=batch_env,\r\n",
    "    compute_target=compute_target, \r\n",
    "    node_count=2,\r\n",
    "    run_invocation_timeout=600\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janguy\\Anaconda3\\envs\\dlresearch\\lib\\site-packages\\azureml\\pipeline\\core\\_parallel_run_step_base.py:591: UserWarning: \n",
      "ParallelRunStep requires azureml-dataset-runtime[fuse,pandas] for tabular dataset.\n",
      "Please add relevant package in CondaDependencies.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "parallel_run_train_step = ParallelRunStep(\r\n",
    "    name='train',\r\n",
    "    inputs=[partitioned_dataset.as_named_input(\"partitioned_tabular_input\")],\r\n",
    "    output=output_dir,\r\n",
    "    parallel_run_config=parallel_run_train_config,\r\n",
    "    allow_reuse=False\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step train [7ec698e8][82a5e60e-3634-4395-9af4-26bd6cbfff70], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 3a46572c-94e0-4f06-9e90-91a664b4f71f\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/3a46572c-94e0-4f06-9e90-91a664b4f71f?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\r\n",
    "from azureml.pipeline.core import Pipeline\r\n",
    "\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallel_run_train_step])\r\n",
    "\r\n",
    "pipeline_run = Experiment(ws, 'Many_Model_Forecast').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 3a46572c-94e0-4f06-9e90-91a664b4f71f\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/3a46572c-94e0-4f06-9e90-91a664b4f71f?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: fa553678-a2e7-480d-aafc-660714e7b37e\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/fa553678-a2e7-480d-aafc-660714e7b37e?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( train ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      "========================================================================================================================\n",
      "2021-06-23T19:49:27Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=92025 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-06-23T19:49:27Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/mounts/workspaceblobstore\n",
      "2021-06-23T19:49:27Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-06-23T19:49:27Z Starting output-watcher...\n",
      "2021-06-23T19:49:27Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-06-23T19:49:28Z Executing 'Copy ACR Details file' on 10.0.0.8\n",
      "2021-06-23T19:49:28Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-06-23T19:49:28Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "2021-06-23T19:49:28Z Copy ACR Details file succeeded on 10.0.0.8. Output: \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_387e6e21562158c0fe995725fde8d3a9\n",
      "4bbfd2c87b75: Pulling fs layer\n",
      "d2e110be24e1: Pulling fs layer\n",
      "889a7173dcfe: Pulling fs layer\n",
      "6a8400ea6a2a: Pulling fs layer\n",
      "f0b10b771c6a: Pulling fs layer\n",
      "7bbe72accfaa: Pulling fs layer\n",
      "61f7682d8d5f: Pulling fs layer\n",
      "ba1e47178ea7: Pulling fs layer\n",
      "e3c979dc924a: Pulling fs layer\n",
      "fbd812f31c7f: Pulling fs layer\n",
      "244b3d13d48e: Pulling fs layer\n",
      "a506a4274325: Pulling fs layer\n",
      "45e70fee18f9: Pulling fs layer\n",
      "abe15f1b9c21: Pulling fs layer\n",
      "9b6f21405c7f: Pulling fs layer\n",
      "3c7280cb23c1: Pulling fs layer\n",
      "86dc933926e1: Pulling fs layer\n",
      "8692fa59005a: Pulling fs layer\n",
      "2a63a57d0f7f: Pulling fs layer\n",
      "09a3d2fff215: Pulling fs layer\n",
      "244b3d13d48e: Waiting\n",
      "a506a4274325: Waiting\n",
      "45e70fee18f9: Waiting\n",
      "abe15f1b9c21: Waiting\n",
      "9b6f21405c7f: Waiting\n",
      "3c7280cb23c1: Waiting\n",
      "86dc933926e1: Waiting\n",
      "8692fa59005a: Waiting\n",
      "2a63a57d0f7f: Waiting\n",
      "09a3d2fff215: Waiting\n",
      "6a8400ea6a2a: Waiting\n",
      "f0b10b771c6a: Waiting\n",
      "61f7682d8d5f: Waiting\n",
      "7bbe72accfaa: Waiting\n",
      "ba1e47178ea7: Waiting\n",
      "e3c979dc924a: Waiting\n",
      "fbd812f31c7f: Waiting\n",
      "d2e110be24e1: Verifying Checksum\n",
      "d2e110be24e1: Download complete\n",
      "889a7173dcfe: Verifying Checksum\n",
      "889a7173dcfe: Download complete\n",
      "4bbfd2c87b75: Verifying Checksum\n",
      "4bbfd2c87b75: Download complete\n",
      "f0b10b771c6a: Verifying Checksum\n",
      "f0b10b771c6a: Download complete\n",
      "7bbe72accfaa: Verifying Checksum\n",
      "7bbe72accfaa: Download complete\n",
      "ba1e47178ea7: Verifying Checksum\n",
      "ba1e47178ea7: Download complete\n",
      "6a8400ea6a2a: Verifying Checksum\n",
      "6a8400ea6a2a: Download complete\n",
      "e3c979dc924a: Verifying Checksum\n",
      "e3c979dc924a: Download complete\n",
      "244b3d13d48e: Verifying Checksum\n",
      "244b3d13d48e: Download complete\n",
      "61f7682d8d5f: Verifying Checksum\n",
      "61f7682d8d5f: Download complete\n",
      "fbd812f31c7f: Verifying Checksum\n",
      "fbd812f31c7f: Download complete\n",
      "45e70fee18f9: Verifying Checksum\n",
      "45e70fee18f9: Download complete\n",
      "a506a4274325: Verifying Checksum\n",
      "a506a4274325: Download complete\n",
      "9b6f21405c7f: Verifying Checksum\n",
      "9b6f21405c7f: Download complete\n",
      "abe15f1b9c21: Verifying Checksum\n",
      "abe15f1b9c21: Download complete\n",
      "86dc933926e1: Verifying Checksum\n",
      "86dc933926e1: Download complete\n",
      "8692fa59005a: Verifying Checksum\n",
      "8692fa59005a: Download complete\n",
      "2a63a57d0f7f: Verifying Checksum\n",
      "2a63a57d0f7f: Download complete\n",
      "09a3d2fff215: Verifying Checksum\n",
      "09a3d2fff215: Download complete\n",
      "3c7280cb23c1: Download complete\n",
      "4bbfd2c87b75: Pull complete\n",
      "d2e110be24e1: Pull complete\n",
      "889a7173dcfe: Pull complete\n",
      "6a8400ea6a2a: Pull complete\n",
      "f0b10b771c6a: Pull complete\n",
      "7bbe72accfaa: Pull complete\n",
      "61f7682d8d5f: Pull complete\n",
      "ba1e47178ea7: Pull complete\n",
      "e3c979dc924a: Pull complete\n",
      "fbd812f31c7f: Pull complete\n",
      "244b3d13d48e: Pull complete\n",
      "a506a4274325: Pull complete\n",
      "45e70fee18f9: Pull complete\n",
      "abe15f1b9c21: Pull complete\n",
      "9b6f21405c7f: Pull complete\n",
      "3c7280cb23c1: Pull complete\n",
      "86dc933926e1: Pull complete\n",
      "8692fa59005a: Pull complete\n",
      "2a63a57d0f7f: Pull complete\n",
      "09a3d2fff215: Pull complete\n",
      "Digest: sha256:cc8c89257f83fb30dd35f0c143f9f00bf7c303751f37dcd39519b2266fe52572\n",
      "Status: Downloaded newer image for ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9:latest\n",
      "ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9:latest\n",
      "2021-06-23T19:50:07Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-06-23T19:50:07Z Check if container fa553678-a2e7-480d-aafc-660714e7b37e already exist exited with 0, \n",
      "\n",
      "79919e63ac9a434c3a7b7bfff66c0ce88da7e26d8754045c6c5e825f018abd43\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/06/23 19:50:24 Starting App Insight Logger for task:  runTaskLet\n",
      "2021/06/23 19:50:24 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      "2021/06/23 19:50:24 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\n",
      "2021/06/23 19:50:24 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
      "[2021-06-23T19:50:24.539115] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\model_train.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'training_output.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/mounts/workspaceblobstore/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input'])\n",
      "Script type = None\n",
      "[2021-06-23T19:50:25.115888] Entering Run History Context Manager.\n",
      "[2021-06-23T19:50:25.899431] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/wd/azureml/fa553678-a2e7-480d-aafc-660714e7b37e\n",
      "[2021-06-23T19:50:25.899893] Preparing to call script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\model_train.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'training_output.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/mounts/workspaceblobstore/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input']\n",
      "[2021-06-23T19:50:25.899929] After variable expansion, calling script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\model_train.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'training_output.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/mounts/workspaceblobstore/azureml/fa553678-a2e7-480d-aafc-660714e7b37e/inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input']\n",
      "\n",
      "2021/06/23 19:50:29 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "\n",
      "\n",
      "[2021-06-23T19:51:27.681836] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "3 items cleaning up...\n",
      "Cleanup took 0.2112717628479004 seconds\n",
      "[2021-06-23T19:51:28.095338] Finished context manager injector.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      "===============================================================================================================\n",
      "[2021-06-23T19:51:30.331002] Entering job release\n",
      "[2021-06-23T19:51:31.372096] Starting job release\n",
      "[2021-06-23T19:51:31.373325] Logging experiment finalizing status in history service.\n",
      "[2021-06-23T19:51:31.373608] job release stage : upload_datastore starting...\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 826\n",
      "[2021-06-23T19:51:31.374509] Entering context manager injector.\n",
      "[2021-06-23T19:51:31.379106] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-06-23T19:51:31.379276] job release stage : execute_job_release starting...\n",
      "[2021-06-23T19:51:31.388433] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-06-23T19:51:31.388476] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-06-23T19:51:31.414165] job release stage : upload_datastore completed...\n",
      "[2021-06-23T19:51:31.535478] job release stage : send_run_telemetry starting...\n",
      "[2021-06-23T19:51:31.563211] get vm size and vm region successfully.\n",
      "[2021-06-23T19:51:31.592046] get compute meta data successfully.\n",
      "[2021-06-23T19:51:31.672207] job release stage : execute_job_release completed...\n",
      "[2021-06-23T19:51:31.861343] post artifact meta request successfully.\n",
      "[2021-06-23T19:51:31.884544] upload compute record artifact successfully.\n",
      "[2021-06-23T19:51:31.884641] job release stage : send_run_telemetry completed...\n",
      "[2021-06-23T19:51:31.884977] Job release is complete\n",
      "\n",
      "StepRun(train) Execution Summary\n",
      "=================================\n",
      "StepRun( train ) Status: Finished\n",
      "{'runId': 'fa553678-a2e7-480d-aafc-660714e7b37e', 'target': 'cpu-cluster', 'status': 'Completed', 'startTimeUtc': '2021-06-23T19:49:22.232584Z', 'endTimeUtc': '2021-06-23T19:51:46.254779Z', 'properties': {'ContentSnapshotId': 'af0eb3e0-3a9d-4667-9657-a12244ef9b8e', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '82a5e60e-3634-4395-9af4-26bd6cbfff70', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '7ec698e8', 'azureml.pipelinerunid': '3a46572c-94e0-4f06-9e90-91a664b4f71f', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.parallelrunstep': 'true'}, 'inputDatasets': [{'dataset': {'id': '2b1f8747-6969-40a8-9842-478ff9274618'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'partitioned_tabular_input', 'mechanism': 'Direct'}}], 'outputDatasets': [], 'runDefinition': {'script': 'driver/amlbi_main.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\model_train.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'training_output.txt', '--output', '$AZUREML_DATAREFERENCE_inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpu-cluster', 'dataReferences': {'inferences': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/fa553678-a2e7-480d-aafc-660714e7b37e/inferences', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'partitioned_tabular_input': {'dataLocation': {'dataset': {'id': '2b1f8747-6969-40a8-9842-478ff9274618', 'name': None, 'version': '8'}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'partitioned_tabular_input', 'pathOnCompute': None, 'overwrite': False}}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 2, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'many_models_environment', 'version': 'Autosave_2021-06-23T19:46:09Z_a2801826', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['sklearn', 'pandas', 'joblib', 'azureml-defaults~=1.31.0', 'azureml-core~=1.31.0', 'azureml-dataprep[fuse]']}], 'name': 'azureml_c89342b67ed7131b61b804dc826e50ae'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210531.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/55_azureml-execution-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt?sv=2019-02-02&sr=b&sig=OWkUWCSiyqHaY2nugpHlFMQpdRY4Xzw%2F0wXgunkwnkY%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/55_azureml-execution-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt?sv=2019-02-02&sr=b&sig=68qJQC8SJrHJj2uVaQJIzL6z9K%2F8wn5NzwcdJs%2F9v4M%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/65_job_prep-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/65_job_prep-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt?sv=2019-02-02&sr=b&sig=19KQi1avC1OhUlbZO7pLbL3Zl%2FAjM0r%2BqcbC0ObOA78%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/65_job_prep-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/65_job_prep-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt?sv=2019-02-02&sr=b&sig=lqCcTIzd5ujTkQgxlFzJYjVAslmywsopRH%2FV4u6AHtU%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=SF24wWDuUfD4PvKXqYYZyZUCUuP3cOZiUsG%2F7Rm8OeM%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/75_job_post-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/75_job_post-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt?sv=2019-02-02&sr=b&sig=e8a5keBMxJVCrsMG%2B6Ks2TkJdr5YE0HsjqbxDs6aSAA%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/75_job_post-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/75_job_post-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt?sv=2019-02-02&sr=b&sig=e1nZ6Fki12BjCwgkG4NodYJNw7UtvHJT%2F6CaY%2FTXdC8%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/process_info.json': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=cBfJ3oNXcyZc4z390BxbeYEyAXQfgVUla6IjshS1W2g%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'azureml-logs/process_status.json': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=bkMi9nPg8ScbdkKgXuoJEgaXYjl6Paz4ic4WP21J0SI%3D&st=2021-06-23T19%3A41%3A35Z&se=2021-06-24T03%3A51%3A35Z&sp=r', 'logs/azureml/110_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/110_azureml.log?sv=2019-02-02&sr=b&sig=FCfWbUI9IycRqsaHeNmmAHsTZCu5KHgKlWur4jRr7wI%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/124_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/124_azureml.log?sv=2019-02-02&sr=b&sig=Srh85CaPxHc%2FoJ9fJ0KmEblkjGHFRCwrJqItMqiLB0o%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=nYr3ZkLp1JtPK8E4RwqraEIqaS6tiyAgrIEEJEYGzM4%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=PTbhuJ5JTNuPu6qA%2BpuZcKEnhxUpS%2FdGfjz7xzFnbzc%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=ZJNNlgOpKlNb49%2BtBjowlIaUo%2FdF5bJKVnH9XLw7TgQ%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=KH6xsYot%2FPjMQgRoEQ5n2oWoeUIDM9naZpi4v2FF2ZA%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=xaCeDo1AdBqci1jhPzRTfbVMIF141G%2F0Z8boxa012lE%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=A9ozBLDjNGy%2FRQLVesSaxtvpoGzT%2F6szx4AVAApPQYQ%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.fa553678-a2e7-480d-aafc-660714e7b37e/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=9UN8pNrqlpGXQQ7rhIa5htOYimnQve2R%2F0ZupjJCNY0%3D&st=2021-06-23T19%3A41%3A33Z&se=2021-06-24T03%3A51%3A33Z&sp=r'}, 'submittedBy': 'James Nguyen'}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '3a46572c-94e0-4f06-9e90-91a664b4f71f', 'status': 'Completed', 'startTimeUtc': '2021-06-23T19:46:06.167671Z', 'endTimeUtc': '2021-06-23T19:51:48.302937Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.3a46572c-94e0-4f06-9e90-91a664b4f71f/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=VfInt8xuM0kimMBcNKjawmp%2BHxTnxarYvN7JGY6XxaI%3D&st=2021-06-23T19%3A41%3A49Z&se=2021-06-24T03%3A51%3A49Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.3a46572c-94e0-4f06-9e90-91a664b4f71f/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=IjiqIbQUJnbL5T1byjMmIaCY3RV3kGcYQL%2BLtNPXpoY%3D&st=2021-06-23T19%3A41%3A49Z&se=2021-06-24T03%3A51%3A49Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.3a46572c-94e0-4f06-9e90-91a664b4f71f/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=IWRTGnqv4RmCBELENGr%2FmMd8L2%2BA5hnK%2BxPSc3o%2Bcig%3D&st=2021-06-23T19%3A41%3A49Z&se=2021-06-24T03%3A51%3A49Z&sp=r'}, 'submittedBy': 'James Nguyen'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train result has  10  rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Brand</th>\n",
       "      <th>mse</th>\n",
       "      <th>mape</th>\n",
       "      <th>rmse</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>1.022011e+07</td>\n",
       "      <td>18.172361</td>\n",
       "      <td>3196.891234</td>\n",
       "      <td>prs_1000_minute.maid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>8.552595e+06</td>\n",
       "      <td>19.463062</td>\n",
       "      <td>2924.481928</td>\n",
       "      <td>prs_1001_tropicana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>1.080181e+07</td>\n",
       "      <td>23.172431</td>\n",
       "      <td>3286.611380</td>\n",
       "      <td>prs_1000_dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>8.920132e+06</td>\n",
       "      <td>20.350161</td>\n",
       "      <td>2986.658951</td>\n",
       "      <td>prs_1001_dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>1.556051e+07</td>\n",
       "      <td>26.075884</td>\n",
       "      <td>3944.680670</td>\n",
       "      <td>prs_1001_minute.maid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1003</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>1.278600e+07</td>\n",
       "      <td>23.512086</td>\n",
       "      <td>3575.751535</td>\n",
       "      <td>prs_1003_dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1002</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>1.069882e+07</td>\n",
       "      <td>20.099309</td>\n",
       "      <td>3270.904584</td>\n",
       "      <td>prs_1002_tropicana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>9.844161e+06</td>\n",
       "      <td>19.846187</td>\n",
       "      <td>3137.540593</td>\n",
       "      <td>prs_1000_tropicana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1002</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>9.404042e+06</td>\n",
       "      <td>23.352023</td>\n",
       "      <td>3066.601004</td>\n",
       "      <td>prs_1002_dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1002</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>6.130244e+06</td>\n",
       "      <td>17.805611</td>\n",
       "      <td>2475.932918</td>\n",
       "      <td>prs_1002_minute.maid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store        Brand           mse       mape         rmse  \\\n",
       "0   1000  minute.maid  1.022011e+07  18.172361  3196.891234   \n",
       "1   1001    tropicana  8.552595e+06  19.463062  2924.481928   \n",
       "2   1000    dominicks  1.080181e+07  23.172431  3286.611380   \n",
       "3   1001    dominicks  8.920132e+06  20.350161  2986.658951   \n",
       "4   1001  minute.maid  1.556051e+07  26.075884  3944.680670   \n",
       "5   1003    dominicks  1.278600e+07  23.512086  3575.751535   \n",
       "6   1002    tropicana  1.069882e+07  20.099309  3270.904584   \n",
       "7   1000    tropicana  9.844161e+06  19.846187  3137.540593   \n",
       "8   1002    dominicks  9.404042e+06  23.352023  3066.601004   \n",
       "9   1002  minute.maid  6.130244e+06  17.805611  2475.932918   \n",
       "\n",
       "             model_name  \n",
       "0  prs_1000_minute.maid  \n",
       "1    prs_1001_tropicana  \n",
       "2    prs_1000_dominicks  \n",
       "3    prs_1001_dominicks  \n",
       "4  prs_1001_minute.maid  \n",
       "5    prs_1003_dominicks  \n",
       "6    prs_1002_tropicana  \n",
       "7    prs_1000_tropicana  \n",
       "8    prs_1002_dominicks  \n",
       "9  prs_1002_minute.maid  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import tempfile\r\n",
    "\r\n",
    "batch_run = pipeline_run.find_step_run(parallel_run_train_step.name)[0]\r\n",
    "batch_output = batch_run.get_output_data(output_dir.name)\r\n",
    "\r\n",
    "target_dir = tempfile.mkdtemp()\r\n",
    "batch_output.download(local_path=target_dir)\r\n",
    "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_train_config.append_row_file_name)\r\n",
    "\r\n",
    "df = pd.read_csv(result_file, delimiter=\" \", header=None)\r\n",
    "\r\n",
    "df.columns = [ \"Store\", \"Brand\", \"mse\", \"mape\", \"rmse\", \"model_name\"]\r\n",
    "print(\"Train result has \", df.shape[0], \" rows\")\r\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the configuration to wrap the inference script\n",
    "The parameter `partition_keys` is a list containing a subset of the dataset partition keys, specifying how is the input dataset partitioned. Each and every possible combination of values of partition_keys will form up a mini-batch. E.g., by specifying `partition_keys=['store', 'brand']` will result in mini-batches like `store=1000 && brand=tropicana`, `store=1000 && brand=dominicks`, `store=1001 && brand=dominicks`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Many_Model_Forecast</td><td>72bb2e27-20f0-42bd-b0eb-e0ef7b57020b</td><td>azureml.StepRun</td><td>Completed</td><td><a href=\"https://ml.azure.com/runs/72bb2e27-20f0-42bd-b0eb-e0ef7b57020b?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: Many_Model_Forecast,\n",
       "Id: 72bb2e27-20f0-42bd-b0eb-e0ef7b57020b,\n",
       "Type: azureml.StepRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Parameter partition_keys: This is an experimental parameter, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\r\n",
    "\r\n",
    "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\r\n",
    "parallel_run_inference_config = ParallelRunConfig(\r\n",
    "    source_directory=scripts_folder,\r\n",
    "    entry_script=inference_script_file,  # the user script to run against each input\r\n",
    "    partition_keys=['Store', 'Brand'],\r\n",
    "    error_threshold= -1,\r\n",
    "    output_action='append_row',\r\n",
    "    append_row_file_name=\"prediction_output.txt\",\r\n",
    "    environment=batch_env,\r\n",
    "    compute_target=compute_target, \r\n",
    "    node_count=2,\r\n",
    "    run_invocation_timeout=600\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_run_inference_step = ParallelRunStep(\r\n",
    "    name='forecast',\r\n",
    "    inputs=[partitioned_dataset.as_named_input(\"partitioned_tabular_input\")],\r\n",
    "    output=output_dir,\r\n",
    "    parallel_run_config=parallel_run_inference_config,\r\n",
    "    allow_reuse=False\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step forecast [f86ab7e3][7dd799f0-a27b-4420-ae09-c4f66c28fe4d], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun dac5b96c-b823-4293-8729-4d146cc0feec\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/dac5b96c-b823-4293-8729-4d146cc0feec?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\r\n",
    "from azureml.pipeline.core import Pipeline\r\n",
    "\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallel_run_inference_step])\r\n",
    "\r\n",
    "pipeline_run = Experiment(ws, 'Many_Model_Forecast').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: dac5b96c-b823-4293-8729-4d146cc0feec\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/dac5b96c-b823-4293-8729-4d146cc0feec?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: d07f1ba7-5d15-47ab-85da-61a8853a986a\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/d07f1ba7-5d15-47ab-85da-61a8853a986a?wsid=/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourcegroups/azureml/workspaces/ws01ent&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( forecast ) Status: NotStarted\n",
      "StepRun( forecast ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      "========================================================================================================================\n",
      "2021-06-23T19:54:59Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=90499 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-06-23T19:54:59Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      "2021-06-23T19:55:00Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-06-23T19:55:00Z Starting output-watcher...\n",
      "2021-06-23T19:55:00Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-06-23T19:55:00Z Executing 'Copy ACR Details file' on 10.0.0.8\n",
      "2021-06-23T19:55:00Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-06-23T19:55:00Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "2021-06-23T19:55:01Z Copy ACR Details file succeeded on 10.0.0.8. Output: \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_387e6e21562158c0fe995725fde8d3a9\n",
      "Digest: sha256:cc8c89257f83fb30dd35f0c143f9f00bf7c303751f37dcd39519b2266fe52572\n",
      "Status: Image is up to date for ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9:latest\n",
      "ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9:latest\n",
      "2021-06-23T19:55:01Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-06-23T19:55:01Z Check if container d07f1ba7-5d15-47ab-85da-61a8853a986a already exist exited with 0, \n",
      "\n",
      "230097b75ad48e9c483515d9d6da970b0dfd281c59e2789407a34effd31fd2d6\n",
      "2021-06-23T19:55:01Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to true \n",
      "2021-06-23T19:55:01Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-4c69bfab358e9cc9652b6a06923ca3ce-06c1bed32355f78c-01 -sshRequired=true] \n",
      "2021/06/23 19:55:02 Starting App Insight Logger for task:  containerSetup\n",
      "2021/06/23 19:55:02 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      "2021/06/23 19:55:02 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/06/23 19:55:02 Starting infiniband setup\n",
      "2021/06/23 19:55:02 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/06/23 19:55:02 Returning Python Version as 3.6\n",
      "2021/06/23 19:55:02 VMSize: standard_d2_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/06/23 19:55:02 VMSize: standard_d2_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021-06-23T19:55:02Z VMSize: standard_d2_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/06/23 19:55:02 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021-06-23T19:55:02Z Not setting up Infiniband in Container\n",
      "2021/06/23 19:55:02 Not setting up Infiniband in Container\n",
      "2021/06/23 19:55:02 Not setting up Infiniband in Container\n",
      "2021/06/23 19:55:02 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/06/23 19:55:02 Returning Python Version as 3.6\n",
      "2021/06/23 19:55:02 Starting setupPasswordLessSSH setup\n",
      "2021/06/23 19:55:02 sshd runtime has already been installed in the container\n",
      "ssh-keygen: /azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "ssh-keygen: /azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "ssh-keygen: generating new host keys: DSA \n",
      "2021/06/23 19:55:02 All App Insights Logs was send successfully\n",
      "2021/06/23 19:55:02 App Insight Client has already been closed\n",
      "2021/06/23 19:55:02 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-06-23T19:55:03Z Starting docker container succeeded.\n",
      "2021-06-23T19:55:12Z Job environment preparation succeeded on 10.0.0.4. Output: \n",
      ">>>   2021/06/23 19:54:59 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/06/23 19:54:59 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      ">>>   2021/06/23 19:54:59 runtime.GOOS linux\n",
      ">>>   2021/06/23 19:54:59 Checking if '/tmp' exists\n",
      ">>>   2021/06/23 19:54:59 Reading dyanamic configs\n",
      ">>>   2021/06/23 19:54:59 Container sas url: https://baiscriptswestus2prod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=71Fsy25bPMdce8Lc7jVPFZbJokMhH4i%2F250OyAEdREA%3D\n",
      ">>>   2021/06/23 19:54:59 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables: not a directory\n",
      ">>>   2021/06/23 19:54:59 [in autoUpgradeFromJobNodeSetup] Is Azsecpack installer on host: true. Is Azsecpack installation enabled: true,\n",
      ">>>   2021/06/23 19:54:59 Starting Azsecpack installation on machine: ce5357c4b25e47d8802d3e0b71095b75000004#72f988bf-86f1-41af-91ab-2d7cd011db47#0e9bace8-7a81-4922-83b5-d995ff706507#azureml#ws01ent#cpu-cluster\n",
      ">>>   2021/06/23 19:54:59 Is Azsecpack enabled: true, GetDisableVsatlsscan: true\n",
      ">>>   2021/06/23 19:54:59 Start preparing environment for azsecpack installation. MachineName is ce5357c4b25e47d8802d3e0b71095b75000004 \n",
      ">>>   \n",
      ">>>   2021/06/23 19:54:59 \n",
      ">>>   2021/06/23 19:54:59 \n",
      ">>>   2021/06/23 19:54:59 OS patching disabled by dynamic configs. Skipping.\n",
      ">>>   2021/06/23 19:54:59 Job: AZ_BATCHAI_JOB_NAME does not turn on the DetonationChamber\n",
      ">>>   2021/06/23 19:54:59 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:54:59 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:54:59 Get GPU count failed with err: The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command., \n",
      ">>>   2021/06/23 19:54:59 AMLComputeXDSEndpoint:  https://westus2-prodk8ds.batchai.core.windows.net\n",
      ">>>   2021/06/23 19:54:59 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/06/23 19:54:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config\n",
      ">>>   2021/06/23 19:54:59 This is not a aml-workstation (compute instance), current offer type: amlcompute. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/06/23 19:54:59 Starting identity responder.\n",
      ">>>   2021/06/23 19:54:59 Starting identity responder.\n",
      ">>>   2021/06/23 19:54:59 Failed to open file /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.IdentityResponder.envlist: open /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.IdentityResponder.envlist: no such file or directory\n",
      ">>>   2021/06/23 19:54:59 Logfile used for identity responder: /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/IdentityResponderLog-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      ">>>   2021/06/23 19:54:59 Logfile used for identity responder: /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/IdentityResponderLog-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      ">>>   2021/06/23 19:54:59 Started Identity Responder for job.\n",
      ">>>   2021/06/23 19:54:59 Started Identity Responder for job.\n",
      ">>>   2021/06/23 19:54:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd\n",
      ">>>   2021/06/23 19:54:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/shared\n",
      ">>>   2021/06/23 19:54:59 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/06/23 19:54:59 Mounting job level file systems\n",
      ">>>   2021/06/23 19:54:59 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts\n",
      ">>>   2021/06/23 19:54:59 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/06/23 19:54:59 Datastore credentials file not found, skipping.\n",
      ">>>   2021/06/23 19:54:59 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.master.runtimesastokens\n",
      ">>>   2021/06/23 19:54:59 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/06/23 19:54:59 No NFS configured\n",
      ">>>   2021/06/23 19:54:59 No Azure File Shares configured\n",
      ">>>   2021/06/23 19:54:59 Mounting blob file systems\n",
      ">>>   2021/06/23 19:54:59 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/06/23 19:54:59 Mounting azureml-blobstore-a167f0bb-0065-4d78-b243-08199690f916 container from ws01ent3218162019 account at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:54:59 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/06/23 19:54:59 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/06/23 19:54:59 Blobfuse cache size set to 90499 MB.\n",
      ">>>   2021/06/23 19:54:59 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=90499 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/06/23 19:54:59 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:54:59 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:54:59 Successfully mounted azureml-blobstore-a167f0bb-0065-4d78-b243-08199690f916 container from ws01ent3218162019 account at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:55:00 Created run_id directory: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a\n",
      ">>>   2021/06/23 19:55:00 No unmanaged file systems configured\n",
      ">>>   2021/06/23 19:55:00 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:00 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:00 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs\n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs\n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/outputs\n",
      ">>>   2021/06/23 19:55:00 Starting output-watcher...\n",
      ">>>   2021/06/23 19:55:00 Single file input dataset is enabled.\n",
      ">>>   2021/06/23 19:55:00 Start to pulling docker image: ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9\n",
      ">>>   2021/06/23 19:55:00 Start pull docker image: ws01ent4ce6f8a0.azurecr.io\n",
      ">>>   2021/06/23 19:55:00 Getting credentials for image ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9 with url ws01ent4ce6f8a0.azurecr.io\n",
      ">>>   2021/06/23 19:55:00 Container registry is ACR.\n",
      ">>>   2021/06/23 19:55:00 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/06/23 19:55:00 Getting ACR Credentials from EMS for environment many_models_environment:Autosave_2021-06-23T19:46:09Z_a2801826\n",
      ">>>   2021/06/23 19:55:00 Requesting XDS for registry details.\n",
      ">>>   2021/06/23 19:55:00 Attempt 1 of http call to https://westus2-prodk8ds.batchai.core.windows.net/hosttoolapi/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/workspaces/ws01ent/clusters/cpu-cluster/nodes/tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d?api-version=2018-02-01\n",
      ">>>   2021/06/23 19:55:00 Got container registry details from credentials service for registry address: ws01ent4ce6f8a0.azurecr.io.\n",
      ">>>   2021/06/23 19:55:00 Writing ACR Details to file...\n",
      ">>>   2021/06/23 19:55:00 Copying ACR Details file to worker nodes...\n",
      ">>>   2021/06/23 19:55:00 Executing 'Copy ACR Details file' on 10.0.0.8\n",
      ">>>   2021/06/23 19:55:00 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/06/23 19:55:00 Executing 'Copy ACR Details file' on 10.0.0.4\n",
      ">>>   2021/06/23 19:55:00 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/06/23 19:55:00 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/06/23 19:55:00 Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   >>>   \n",
      ">>>   >>>   \n",
      ">>>   2021/06/23 19:55:01 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/06/23 19:55:01 Copy ACR Details file succeeded on 10.0.0.8. Output: \n",
      ">>>   >>>   \n",
      ">>>   2021/06/23 19:55:01 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/06/23 19:55:01 EMS returned ws01ent4ce6f8a0.azurecr.io for environment many_models_environment\n",
      ">>>   2021/06/23 19:55:01 Save docker credentials for image ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9 in /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/docker_login_429702F5CDF77E35\n",
      ">>>   2021/06/23 19:55:01 Start login to the docker registry\n",
      ">>>   2021/06/23 19:55:01 Successfully logged into the docker registry.\n",
      ">>>   2021/06/23 19:55:01 Start run pull docker image command\n",
      ">>>   2021/06/23 19:55:01 Pull docker image succeeded.\n",
      ">>>   2021/06/23 19:55:01 Removed docker config dir /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/docker_login_429702F5CDF77E35\n",
      ">>>   2021/06/23 19:55:01 Pull docker image time: 1.205463395s\n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:01 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:01 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:01 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:01 Setting the memory limit for docker container to be 6618 MB\n",
      ">>>   2021/06/23 19:55:01 The env variable file size is 41480 bytes\n",
      ">>>   2021/06/23 19:55:01 Creating parent cgroup 'd07f1ba7-5d15-47ab-85da-61a8853a986a' for Containers used in Job\n",
      ">>>   2021/06/23 19:55:01 Add parent cgroup 'd07f1ba7-5d15-47ab-85da-61a8853a986a' to container 'd07f1ba7-5d15-47ab-85da-61a8853a986a'\n",
      ">>>   2021/06/23 19:55:01 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/06/23 19:55:01 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,d07f1ba7-5d15-47ab-85da-61a8853a986a,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-m,6618m,-v,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.envlist,--cgroup-parent=/d07f1ba7-5d15-47ab-85da-61a8853a986a/,--shm-size,2g\n",
      ">>>   2021/06/23 19:55:01 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/06/23 19:55:01 the binding /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a \n",
      ">>>   2021/06/23 19:55:01 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,d07f1ba7-5d15-47ab-85da-61a8853a986a,-m,6618m,-w,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.envlist,--cgroup-parent=/d07f1ba7-5d15-47ab-85da-61a8853a986a/,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs\n",
      ">>>   2021/06/23 19:55:01 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name d07f1ba7-5d15-47ab-85da-61a8853a986a -m 6618m -w /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.envlist --cgroup-parent=/d07f1ba7-5d15-47ab-85da-61a8853a986a/ --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a -v /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd -v /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs -d -it --privileged --net=host ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9\n",
      ">>>   2021/06/23 19:55:01 Check if container d07f1ba7-5d15-47ab-85da-61a8853a986a already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:01 Check if container d07f1ba7-5d15-47ab-85da-61a8853a986a already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:01 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to true \n",
      ">>>   2021/06/23 19:55:01 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to true \n",
      ">>>   2021/06/23 19:55:01 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-4c69bfab358e9cc9652b6a06923ca3ce-06c1bed32355f78c-01 -sshRequired=true] \n",
      ">>>   2021/06/23 19:55:01 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-4c69bfab358e9cc9652b6a06923ca3ce-06c1bed32355f78c-01 -sshRequired=true] \n",
      ">>>   2021/06/23 19:55:03 Starting docker container succeeded.\n",
      ">>>   2021/06/23 19:55:03 Starting docker container succeeded.\n",
      ">>>   2021/06/23 19:55:03 Disk space after starting docker container: 93004MB\n",
      ">>>   2021/06/23 19:55:03 Begin execution of runSpecialJobTask\n",
      ">>>   2021/06/23 19:55:03 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs\n",
      ">>>   2021/06/23 19:55:03 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"af0eb3e0-3a9d-4667-9657-a12244ef9b8e\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null},{\"Id\":\"31d582e6-554f-4a04-8cc5-64befe2015c6\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/23 19:55:03 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs/65_job_prep-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      ">>>   2021/06/23 19:55:03 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs/65_job_prep-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      ">>>   2021/06/23 19:55:03 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a;/azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"af0eb3e0-3a9d-4667-9657-a12244ef9b8e\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null},{\"Id\":\"31d582e6-554f-4a04-8cc5-64befe2015c6\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/23 19:55:03 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/06/23 19:55:03 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-4c69bfab358e9cc9652b6a06923ca3ce-088e9c777e862414-01 -t d07f1ba7-5d15-47ab-85da-61a8853a986a bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a;/azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"af0eb3e0-3a9d-4667-9657-a12244ef9b8e\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null},{\"Id\":\"31d582e6-554f-4a04-8cc5-64befe2015c6\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/23 19:55:04 Attempt 1 of http call to https://westus2.api.azureml.ms/history/v1.0/private/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/providers/Microsoft.MachineLearningServices/workspaces/ws01ent/runs/d07f1ba7-5d15-47ab-85da-61a8853a986a/spans\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:03.839378] Entering job preparation.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.739629] Starting job preparation.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.739663] Extracting the control code.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.740045] Starting extract_project.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.740125] Starting to extract zip file.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.765435] Finished extracting zip file.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.768173] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.768224] Start fetching snapshots.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.768269] Start fetching snapshot.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:05.768292] Retrieving project from snapshot: af0eb3e0-3a9d-4667-9657-a12244ef9b8e\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 75\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:06.448247] Finished fetching snapshot.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:06.448290] Start fetching snapshot.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:06.448301] Retrieving project from snapshot: 31d582e6-554f-4a04-8cc5-64befe2015c6\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:11.596968] Finished fetching snapshot.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:11.597003] Finished fetching snapshots.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:11.597020] Finished extract_project.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:11.597107] Finished fetching and extracting the control code.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:11.601943] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:11.611351] Start run_history_prep.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:11.638799] Entering context manager injector.\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: Acquired lockfile /tmp/d07f1ba7-5d15-47ab-85da-61a8853a986a-datastore.lock to downloading input data references\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:12.307664] downloadDataStore completed\n",
      ">>>   2021/06/23 19:55:12 runSpecialJobTask: preparation: [2021-06-23T19:55:12.310759] Job preparation is complete.\n",
      ">>>   2021/06/23 19:55:12 Execution of runSpecialJobTask completed\n",
      ">>>   2021/06/23 19:55:12 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 3\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/06/23 19:55:12 Process Exiting with Code:  0\n",
      ">>>   2021/06/23 19:55:12 All App Insights Logs was send successfully\n",
      ">>>   \n",
      "2021-06-23T19:55:14Z Job environment preparation succeeded on 10.0.0.8. Output: \n",
      ">>>   2021/06/23 19:55:00 Attempt 1 of http call to http://10.0.0.4:16384/getInstrumentationKey\n",
      ">>>   2021/06/23 19:55:00 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/06/23 19:55:00 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      ">>>   2021/06/23 19:55:00 runtime.GOOS linux\n",
      ">>>   2021/06/23 19:55:00 Checking if '/tmp' exists\n",
      ">>>   2021/06/23 19:55:00 Reading dyanamic configs\n",
      ">>>   2021/06/23 19:55:00 Container sas url: https://baiscriptswestus2prod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=71Fsy25bPMdce8Lc7jVPFZbJokMhH4i%2F250OyAEdREA%3D\n",
      ">>>   2021/06/23 19:55:00 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables: not a directory\n",
      ">>>   2021/06/23 19:55:00 [in autoUpgradeFromJobNodeSetup] Is Azsecpack installer on host: true. Is Azsecpack installation enabled: true,\n",
      ">>>   2021/06/23 19:55:00 Starting Azsecpack installation on machine: ce5357c4b25e47d8802d3e0b71095b75000006#72f988bf-86f1-41af-91ab-2d7cd011db47#0e9bace8-7a81-4922-83b5-d995ff706507#azureml#ws01ent#cpu-cluster\n",
      ">>>   2021/06/23 19:55:00 Is Azsecpack enabled: true, GetDisableVsatlsscan: true\n",
      ">>>   2021/06/23 19:55:00 Start preparing environment for azsecpack installation. MachineName is ce5357c4b25e47d8802d3e0b71095b75000006 \n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:00 \n",
      ">>>   2021/06/23 19:55:00 \n",
      ">>>   2021/06/23 19:55:00 OS patching disabled by dynamic configs. Skipping.\n",
      ">>>   2021/06/23 19:55:00 Job: AZ_BATCHAI_JOB_NAME does not turn on the DetonationChamber\n",
      ">>>   2021/06/23 19:55:00 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:00 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:00 Get GPU count failed with err: The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command., \n",
      ">>>   2021/06/23 19:55:00 AMLComputeXDSEndpoint:  https://westus2-prodk8ds.batchai.core.windows.net\n",
      ">>>   2021/06/23 19:55:00 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config\n",
      ">>>   2021/06/23 19:55:00 This is not a aml-workstation (compute instance), current offer type: amlcompute. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/06/23 19:55:00 Starting identity responder.\n",
      ">>>   2021/06/23 19:55:00 Starting identity responder.\n",
      ">>>   2021/06/23 19:55:00 Failed to open file /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.IdentityResponder.envlist: open /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.IdentityResponder.envlist: no such file or directory\n",
      ">>>   2021/06/23 19:55:00 Logfile used for identity responder: /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/IdentityResponderLog-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt\n",
      ">>>   2021/06/23 19:55:00 Logfile used for identity responder: /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/IdentityResponderLog-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt\n",
      ">>>   2021/06/23 19:55:00 Started Identity Responder for job.\n",
      ">>>   2021/06/23 19:55:00 Started Identity Responder for job.\n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd\n",
      ">>>   2021/06/23 19:55:00 No NFS present in mount volumes\n",
      ">>>   2021/06/23 19:55:00 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/06/23 19:55:00 Mounting job level file systems\n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts\n",
      ">>>   2021/06/23 19:55:00 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/06/23 19:55:00 Datastore credentials file not found, skipping.\n",
      ">>>   2021/06/23 19:55:00 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.master.runtimesastokens\n",
      ">>>   2021/06/23 19:55:00 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/06/23 19:55:00 No NFS configured\n",
      ">>>   2021/06/23 19:55:00 No Azure File Shares configured\n",
      ">>>   2021/06/23 19:55:00 Mounting blob file systems\n",
      ">>>   2021/06/23 19:55:00 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/06/23 19:55:00 Mounting azureml-blobstore-a167f0bb-0065-4d78-b243-08199690f916 container from ws01ent3218162019 account at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:55:00 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/06/23 19:55:00 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/06/23 19:55:00 Blobfuse cache size set to 90714 MB.\n",
      ">>>   2021/06/23 19:55:00 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=90714 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/06/23 19:55:00 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:55:00 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:55:00 Successfully mounted azureml-blobstore-a167f0bb-0065-4d78-b243-08199690f916 container from ws01ent3218162019 account at /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore\n",
      ">>>   2021/06/23 19:55:00 Created run_id directory: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a\n",
      ">>>   2021/06/23 19:55:00 No unmanaged file systems configured\n",
      ">>>   2021/06/23 19:55:00 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:00 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:00 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs\n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs\n",
      ">>>   2021/06/23 19:55:00 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/outputs\n",
      ">>>   2021/06/23 19:55:00 Starting output-watcher...\n",
      ">>>   2021/06/23 19:55:00 Single file input dataset is enabled.\n",
      ">>>   2021/06/23 19:55:00 Start to pulling docker image: ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9\n",
      ">>>   2021/06/23 19:55:00 Start pull docker image: ws01ent4ce6f8a0.azurecr.io\n",
      ">>>   2021/06/23 19:55:00 Getting credentials for image ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9 with url ws01ent4ce6f8a0.azurecr.io\n",
      ">>>   2021/06/23 19:55:00 Container registry is ACR.\n",
      ">>>   2021/06/23 19:55:00 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/06/23 19:55:00 Getting ACR Credentials from EMS for environment many_models_environment:Autosave_2021-06-23T19:46:09Z_a2801826\n",
      ">>>   2021/06/23 19:55:00 Read ACR details from file.\n",
      ">>>   2021/06/23 19:55:00 Reading ACR credentials from file...\n",
      ">>>   2021/06/23 19:55:00 Unable to parse ACR Details file with err: open /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.amlcompute.acrdetailsmany_models_environment: no such file or directory\n",
      ">>>   2021/06/23 19:55:00 Failed to unmarshal containerRegistry from API: unexpected end of JSON input\n",
      ">>>   2021/06/23 19:55:00 EncryptedDockerRegistryPassword is empty.\n",
      ">>>   2021/06/23 19:55:00 Getting ACR details from file failed, making an XDS call.\n",
      ">>>   2021/06/23 19:55:00 Failed to read hosttool JSON file: open /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd/hosttools.json: no such file or directory\n",
      ">>>   2021/06/23 19:55:00 Requesting XDS for registry details.\n",
      ">>>   2021/06/23 19:55:00 Attempt 1 of http call to https://westus2-prodk8ds.batchai.core.windows.net/hosttoolapi/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/workspaces/ws01ent/clusters/cpu-cluster/nodes/tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d?api-version=2018-02-01\n",
      ">>>   2021/06/23 19:55:01 Got container registry details from credentials service for registry address: ws01ent4ce6f8a0.azurecr.io.\n",
      ">>>   2021/06/23 19:55:01 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/06/23 19:55:01 EMS returned ws01ent4ce6f8a0.azurecr.io for environment many_models_environment\n",
      ">>>   2021/06/23 19:55:01 Save docker credentials for image ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9 in /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/docker_login_E5570F788A5BCDB0\n",
      ">>>   2021/06/23 19:55:01 Start login to the docker registry\n",
      ">>>   2021/06/23 19:55:01 Successfully logged into the docker registry.\n",
      ">>>   2021/06/23 19:55:01 Start run pull docker image command\n",
      ">>>   2021/06/23 19:55:01 Pull docker image succeeded.\n",
      ">>>   2021/06/23 19:55:01 Removed docker config dir /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/docker_login_E5570F788A5BCDB0\n",
      ">>>   2021/06/23 19:55:01 Pull docker image time: 1.069723995s\n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:01 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:01 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:01 The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/06/23 19:55:01 Setting the memory limit for docker container to be 6618 MB\n",
      ">>>   2021/06/23 19:55:01 The env variable file size is 35350 bytes\n",
      ">>>   2021/06/23 19:55:01 Creating parent cgroup 'd07f1ba7-5d15-47ab-85da-61a8853a986a' for Containers used in Job\n",
      ">>>   2021/06/23 19:55:01 Add parent cgroup 'd07f1ba7-5d15-47ab-85da-61a8853a986a' to container 'd07f1ba7-5d15-47ab-85da-61a8853a986a'\n",
      ">>>   2021/06/23 19:55:01 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/06/23 19:55:01 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,d07f1ba7-5d15-47ab-85da-61a8853a986a,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-m,6618m,-v,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.envlist,--cgroup-parent=/d07f1ba7-5d15-47ab-85da-61a8853a986a/,--shm-size,2g\n",
      ">>>   2021/06/23 19:55:01 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/06/23 19:55:01 the binding /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a \n",
      ">>>   2021/06/23 19:55:01 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,d07f1ba7-5d15-47ab-85da-61a8853a986a,-m,6618m,-w,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.envlist,--cgroup-parent=/d07f1ba7-5d15-47ab-85da-61a8853a986a/,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd,-v,/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs\n",
      ">>>   2021/06/23 19:55:01 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name d07f1ba7-5d15-47ab-85da-61a8853a986a -m 6618m -w /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/config/.batchai.envlist --cgroup-parent=/d07f1ba7-5d15-47ab-85da-61a8853a986a/ --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a:/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a -v /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd -v /mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs:/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/certs -d -it --privileged --net=host ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9\n",
      ">>>   2021/06/23 19:55:02 Check if container d07f1ba7-5d15-47ab-85da-61a8853a986a already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:02 Check if container d07f1ba7-5d15-47ab-85da-61a8853a986a already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:02 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to true \n",
      ">>>   2021/06/23 19:55:02 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to true \n",
      ">>>   2021/06/23 19:55:02 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-4c69bfab358e9cc9652b6a06923ca3ce-36a142e1620bc35f-01 -sshRequired=true] \n",
      ">>>   2021/06/23 19:55:02 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-4c69bfab358e9cc9652b6a06923ca3ce-36a142e1620bc35f-01 -sshRequired=true] \n",
      ">>>   2021/06/23 19:55:03 Waiting for sshd in the container to become available. Last error is exit status 255, output is ssh: connect to host 10.0.0.8 port 23: Connection refused\n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:03 docker ps: CONTAINER ID        IMAGE                                                                         COMMAND             CREATED             STATUS              PORTS               NAMES\n",
      ">>>   edded5c6d0d1        ws01ent4ce6f8a0.azurecr.io/azureml/azureml_387e6e21562158c0fe995725fde8d3a9   \"bash\"              1 second ago        Up 1 second                             d07f1ba7-5d15-47ab-85da-61a8853a986a\n",
      ">>>   , error: <nil>\n",
      ">>>   2021/06/23 19:55:03 ps in container:   PID TTY      STAT   TIME COMMAND\n",
      ">>>      48 ?        Rs     0:00 ps -afx\n",
      ">>>      42 ?        Ss     0:00 /usr/sbin/sshd -D -p 23\n",
      ">>>       1 pts/0    Ss+    0:00 bash\n",
      ">>>   , error: <nil>\n",
      ">>>   2021/06/23 19:55:05 Starting docker container succeeded.\n",
      ">>>   2021/06/23 19:55:05 Starting docker container succeeded.\n",
      ">>>   2021/06/23 19:55:05 Disk space after starting docker container: 93220MB\n",
      ">>>   2021/06/23 19:55:05 Begin execution of runSpecialJobTask\n",
      ">>>   2021/06/23 19:55:05 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs\n",
      ">>>   2021/06/23 19:55:05 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"af0eb3e0-3a9d-4667-9657-a12244ef9b8e\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null},{\"Id\":\"31d582e6-554f-4a04-8cc5-64befe2015c6\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/23 19:55:05 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs/65_job_prep-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt\n",
      ">>>   2021/06/23 19:55:05 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml_compute_logs/65_job_prep-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt\n",
      ">>>   2021/06/23 19:55:05 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a;/azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"af0eb3e0-3a9d-4667-9657-a12244ef9b8e\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null},{\"Id\":\"31d582e6-554f-4a04-8cc5-64befe2015c6\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/23 19:55:05 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/06/23 19:55:05 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-4c69bfab358e9cc9652b6a06923ca3ce-ccb9797eff3af20b-01 -t d07f1ba7-5d15-47ab-85da-61a8853a986a bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/386fc354-a5fb-45c3-b0bf-a07597feb17e/job-1/d07f1ba7-5d15-47ab-8_cde06682-9fa7-487c-89c5-d805055fe4bb/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a;/azureml-envs/azureml_c89342b67ed7131b61b804dc826e50ae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"af0eb3e0-3a9d-4667-9657-a12244ef9b8e\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null},{\"Id\":\"31d582e6-554f-4a04-8cc5-64befe2015c6\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/06/23 19:55:05 Attempt 1 of http call to https://westus2.api.azureml.ms/history/v1.0/private/subscriptions/0e9bace8-7a81-4922-83b5-d995ff706507/resourceGroups/azureml/providers/Microsoft.MachineLearningServices/workspaces/ws01ent/runs/d07f1ba7-5d15-47ab-85da-61a8853a986a/spans\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:05.650567] Entering job preparation.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.966949] Starting job preparation.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.966985] Extracting the control code.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.967738] Starting extract_project.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.967789] Starting to extract zip file.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.989173] Finished extracting zip file.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.992661] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.992705] Start fetching snapshots.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.992736] Start fetching snapshot.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:06.992760] Retrieving project from snapshot: af0eb3e0-3a9d-4667-9657-a12244ef9b8e\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 84\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:07.448277] Finished fetching snapshot.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:07.448323] Start fetching snapshot.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:07.448347] Retrieving project from snapshot: 31d582e6-554f-4a04-8cc5-64befe2015c6\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:12.814986] Finished fetching snapshot.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:12.815043] Finished fetching snapshots.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:12.815059] Finished extract_project.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:12.815356] Finished fetching and extracting the control code.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:12.819280] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:12.820947] Start run_history_prep.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:12.826705] Entering context manager injector.\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: Acquired lockfile /tmp/d07f1ba7-5d15-47ab-85da-61a8853a986a-datastore.lock to downloading input data references\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:13.335529] downloadDataStore completed\n",
      ">>>   2021/06/23 19:55:13 runSpecialJobTask: preparation: [2021-06-23T19:55:13.338885] Job preparation is complete.\n",
      ">>>   2021/06/23 19:55:13 Execution of runSpecialJobTask completed\n",
      ">>>   2021/06/23 19:55:13 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 3\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/06/23 19:55:13 Process Exiting with Code:  0\n",
      ">>>   2021/06/23 19:55:14 All App Insights Logs was send successfully\n",
      ">>>   \n",
      "2021-06-23T19:55:14Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-06-23T19:55:14Z The vmsize standard_d2_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-06-23T19:55:14Z launching Custom job\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/06/23 19:55:14 Starting App Insight Logger for task:  runTaskLet\n",
      "2021/06/23 19:55:14 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612\n",
      "2021/06/23 19:55:14 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\n",
      "2021/06/23 19:55:14 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
      "[2021-06-23T19:55:14.669928] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\prediction.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'prediction_output.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input'])\n",
      "Script type = None\n",
      "[2021-06-23T19:55:15.369084] Entering Run History Context Manager.\n",
      "[2021-06-23T19:55:16.272206] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/wd/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a\n",
      "[2021-06-23T19:55:16.272509] Preparing to call script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\prediction.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'prediction_output.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input']\n",
      "[2021-06-23T19:55:16.272545] After variable expansion, calling script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\prediction.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'prediction_output.txt', '--output', '/mnt/batch/tasks/shared/LS_root/jobs/ws01ent/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/mounts/workspaceblobstore/azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input']\n",
      "\n",
      "2021/06/23 19:55:19 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "\n",
      "\n",
      "[2021-06-23T19:56:09.788937] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "3 items cleaning up...\n",
      "Cleanup took 0.2553904056549072 seconds\n",
      "[2021-06-23T19:56:10.198630] Finished context manager injector.\n",
      "2021/06/23 19:56:11 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
      "2021/06/23 19:56:11 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 2\n",
      "FilteredData: 0.\n",
      "2021/06/23 19:56:11 Process Exiting with Code:  0\n",
      "2021/06/23 19:56:11 All App Insights Logs was send successfully\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt\n",
      "===============================================================================================================\n",
      "[2021-06-23T19:56:14.691569] Entering job release\n",
      "[2021-06-23T19:56:15.719189] Starting job release\n",
      "[2021-06-23T19:56:15.719920] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 938\n",
      "[2021-06-23T19:56:15.723497] job release stage : upload_datastore starting...\n",
      "[2021-06-23T19:56:15.731258] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-06-23T19:56:15.731855] Entering context manager injector.\n",
      "[2021-06-23T19:56:15.735866] job release stage : execute_job_release starting...\n",
      "[2021-06-23T19:56:15.738564] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-06-23T19:56:15.739274] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-06-23T19:56:15.751276] job release stage : upload_datastore completed...\n",
      "[2021-06-23T19:56:15.822343] job release stage : send_run_telemetry starting...\n",
      "[2021-06-23T19:56:15.843950] get vm size and vm region successfully.\n",
      "[2021-06-23T19:56:15.853414] get compute meta data successfully.\n",
      "[2021-06-23T19:56:16.004954] job release stage : execute_job_release completed...\n",
      "[2021-06-23T19:56:16.050214] post artifact meta request successfully.\n",
      "[2021-06-23T19:56:16.105536] upload compute record artifact successfully.\n",
      "[2021-06-23T19:56:16.105622] job release stage : send_run_telemetry completed...\n",
      "[2021-06-23T19:56:16.106160] Job release is complete\n",
      "\n",
      "StepRun(forecast) Execution Summary\n",
      "====================================\n",
      "StepRun( forecast ) Status: Finished\n",
      "{'runId': 'd07f1ba7-5d15-47ab-85da-61a8853a986a', 'target': 'cpu-cluster', 'status': 'Completed', 'startTimeUtc': '2021-06-23T19:54:56.409362Z', 'endTimeUtc': '2021-06-23T19:56:30.486291Z', 'properties': {'ContentSnapshotId': 'af0eb3e0-3a9d-4667-9657-a12244ef9b8e', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '7dd799f0-a27b-4420-ae09-c4f66c28fe4d', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'f86ab7e3', 'azureml.pipelinerunid': 'dac5b96c-b823-4293-8729-4d146cc0feec', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.parallelrunstep': 'true'}, 'inputDatasets': [{'dataset': {'id': '2b1f8747-6969-40a8-9842-478ff9274618'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'partitioned_tabular_input', 'mechanism': 'Direct'}}], 'outputDatasets': [], 'runDefinition': {'script': 'driver/amlbi_main.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--client_sdk_version', '1.31.0', '--scoring_module_name', 'aml_prs\\\\prediction.py', '--mini_batch_size', '1048576', '--error_threshold', '-1', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '600', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--append_row_file_name', 'prediction_output.txt', '--output', '$AZUREML_DATAREFERENCE_inferences', '--partition_keys', '[\"Store\", \"Brand\"]', '--input_ds_0', 'partitioned_tabular_input'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpu-cluster', 'dataReferences': {'inferences': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/d07f1ba7-5d15-47ab-85da-61a8853a986a/inferences', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'partitioned_tabular_input': {'dataLocation': {'dataset': {'id': '2b1f8747-6969-40a8-9842-478ff9274618', 'name': None, 'version': '8'}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'partitioned_tabular_input', 'pathOnCompute': None, 'overwrite': False}}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 2, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'many_models_environment', 'version': 'Autosave_2021-06-23T19:46:09Z_a2801826', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['sklearn', 'pandas', 'joblib', 'azureml-defaults~=1.31.0', 'azureml-core~=1.31.0', 'azureml-dataprep[fuse]']}], 'name': 'azureml_c89342b67ed7131b61b804dc826e50ae'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210531.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/55_azureml-execution-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt?sv=2019-02-02&sr=b&sig=nkRaZaad8XsoFXuwxz4UMm90xI%2Bjse9hK9YCGynA5As%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/55_azureml-execution-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt?sv=2019-02-02&sr=b&sig=sr%2BMT%2FBFXYL5YtIFs5FVykswYOHhU8pxkEWvq%2Ff6i8U%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/65_job_prep-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/65_job_prep-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt?sv=2019-02-02&sr=b&sig=nRha4Wc%2Bf%2Blc8i92RZzEQclU%2BLHRlsa64KH48KC0M1s%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/65_job_prep-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/65_job_prep-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt?sv=2019-02-02&sr=b&sig=d9wiDbjIF0axpsxVond6iXKZdyDr4a3aPbcGeoq345Y%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=NJvLpprrqVefI5i7zb3UJKRjYN%2FzKYXB%2Ft7%2BovI4%2FlY%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/75_job_post-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/75_job_post-tvmps_9dad2b059b79040f8ad5ca8131190ad001a3d68c0a40c6795d0e83cf5f33f2c0_d.txt?sv=2019-02-02&sr=b&sig=J4lEQa4izPqr4FmG1Onvmd2v9%2FwbEr%2FG1mRud1hkdPU%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/75_job_post-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/75_job_post-tvmps_b017c3b2a7d86c452f9ae00e547ae3165f121d6f77c41caf83e70eb2f9286752_d.txt?sv=2019-02-02&sr=b&sig=ou6paMTUMoant1rduNQRb1%2B92CqIlyx7XspT8YZg%2BfY%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/process_info.json': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=kc1vlH3qZJa4NAZ6DtwXs6NEIACweI1fiK%2B9fvCNj%2FI%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'azureml-logs/process_status.json': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=pN56Byb%2BQTccTOhe6s1jSvq9OXdKiAjRY9kXpCKydi4%3D&st=2021-06-23T19%3A46%3A19Z&se=2021-06-24T03%3A56%3A19Z&sp=r', 'logs/azureml/123_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/123_azureml.log?sv=2019-02-02&sr=b&sig=SOm%2BhFpHNJInuOapPt8JfGnogW0V1JYWrHqjcI8mk9I%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/128_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/128_azureml.log?sv=2019-02-02&sr=b&sig=jRpSWvRlpmCobNbdj%2BgA5v%2FhBqr4B9U21IlW3KPjI%2B0%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=rt5w4QVEruEFbPy1GIgDVs5TKeINo767bc51FHbCJws%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=y6Pev40U5m0PSX7%2F9kCs5FGf3cDz7UZ7%2FAm%2BHX2f5Qc%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=V1V1%2BdUSoYlY86jqx%2B6sMalcOVd3%2BYzh2YzLpESM3pQ%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=wMfOaNkU5RekXAH6lG7k5BdyuW1XC4YSXiawQdd%2BvpI%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=lFTX2CHS6cPtx1ZmO1SK6bjgyS0d6Rq3Ol3fw7AVDa4%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=EVR9EbUDTkAv73pYoB31k00PdPK4ufVSdtarQuyf7DE%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.d07f1ba7-5d15-47ab-85da-61a8853a986a/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=q%2F75gDb4RlM7gF6ATmi28CXlcp2nPmNPMn4uenLdk9g%3D&st=2021-06-23T19%3A46%3A18Z&se=2021-06-24T03%3A56%3A18Z&sp=r'}, 'submittedBy': 'James Nguyen'}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'dac5b96c-b823-4293-8729-4d146cc0feec', 'status': 'Completed', 'startTimeUtc': '2021-06-23T19:54:38.761609Z', 'endTimeUtc': '2021-06-23T19:56:33.073322Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.dac5b96c-b823-4293-8729-4d146cc0feec/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=ygQ6NTB3zie9RXscKjC5ebv3C3JfQKAoE22k2MLa8cA%3D&st=2021-06-23T19%3A46%3A34Z&se=2021-06-24T03%3A56%3A34Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.dac5b96c-b823-4293-8729-4d146cc0feec/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=nUQr7u9U600xdz%2F5aPCUJKj6XyDDL3MsR10nYTLNYLQ%3D&st=2021-06-23T19%3A46%3A34Z&se=2021-06-24T03%3A56%3A34Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://ws01ent3218162019.blob.core.windows.net/azureml/ExperimentRun/dcid.dac5b96c-b823-4293-8729-4d146cc0feec/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=CvNRabEfnpU4H3hhjN7ctl8CF9fzbife2BimC2PiyJ8%3D&st=2021-06-23T19%3A46%3A34Z&se=2021-06-24T03%3A56%3A34Z&sp=r'}, 'submittedBy': 'James Nguyen'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the prediction results\n",
    "In the total_income.py file above you can see that the ResultList with the filename and the prediction result gets returned. These are written to the DataStore specified in the PipelineData object as the output data, which in this case is called inferences. This containers the outputs from all of the worker nodes used in the compute cluster. You can download this data to view the results ... below just filters to the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction has  1210  rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekStarting</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Store</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-06-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-07-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-07-12</td>\n",
       "      <td>13554.387879</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1990-07-19</td>\n",
       "      <td>15375.931186</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1990-07-26</td>\n",
       "      <td>15243.409687</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1990-08-02</td>\n",
       "      <td>13649.643619</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1990-08-09</td>\n",
       "      <td>14017.612778</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1990-08-16</td>\n",
       "      <td>15976.467391</td>\n",
       "      <td>1001</td>\n",
       "      <td>dominicks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WeekStarting    Prediction  Store      Brand\n",
       "0   1990-06-14           NaN   1001  dominicks\n",
       "1   1990-06-21           NaN   1001  dominicks\n",
       "2   1990-06-28           NaN   1001  dominicks\n",
       "3   1990-07-05           NaN   1001  dominicks\n",
       "4   1990-07-12  13554.387879   1001  dominicks\n",
       "5   1990-07-19  15375.931186   1001  dominicks\n",
       "6   1990-07-26  15243.409687   1001  dominicks\n",
       "7   1990-08-02  13649.643619   1001  dominicks\n",
       "8   1990-08-09  14017.612778   1001  dominicks\n",
       "9   1990-08-16  15976.467391   1001  dominicks"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import tempfile\r\n",
    "\r\n",
    "batch_run = pipeline_run.find_step_run(parallel_run_inference_step.name)[0]\r\n",
    "batch_output = batch_run.get_output_data(output_dir.name)\r\n",
    "\r\n",
    "target_dir = tempfile.mkdtemp()\r\n",
    "batch_output.download(local_path=target_dir)\r\n",
    "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_config.append_row_file_name)\r\n",
    "\r\n",
    "df = pd.read_csv(result_file, delimiter=\" \", header=None)\r\n",
    "df.columns = [\"WeekStarting\", \"Prediction\", \"Store\", \"Brand\"]\r\n",
    "print(\"Prediction has \", df.shape[0], \" rows\")\r\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "pansav"
   },
   {
    "name": "tracych"
   },
   {
    "name": "migu"
   }
  ],
  "category": "Other notebooks",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "OJ Sales Data"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "None"
  ],
  "friendly_name": "Batch inferencing OJ Sales Data partitioned by column using ParallelRunStep",
  "index_order": 1,
  "interpreter": {
   "hash": "f7f364c9551711cd4699acda32e0312c3edab483ae246bf330de758088cecccb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dlresearch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}